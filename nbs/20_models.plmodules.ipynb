{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.plmodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from grade_classif.data.dataset import ImageClassifDataset, NormDataset\n",
    "from grade_classif.data.transforms import get_transforms\n",
    "from grade_classif.data.utils import show_img\n",
    "from grade_classif.models.utils import named_leaf_modules, get_sizes, get_num_features\n",
    "from grade_classif.models.modules import DynamicUnet, bn_drop_lin\n",
    "from grade_classif.core import ifnone\n",
    "import timm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_loss(loss_name, weight, reduction, device='cpu'):\n",
    "    if loss_name == 'cross-entropy':\n",
    "        loss = nn.CrossEntropyLoss(torch.tensor([weight, 1.], device=device), reduction=reduction)\n",
    "    if loss_name == 'mse':\n",
    "        loss = nn.MSELoss(reduction=reduction)\n",
    "    return loss.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_scheduler(opt, name, total_steps, lr):\n",
    "    if name == 'one-cycle':\n",
    "        sched = OneCycleLR(opt, lr, total_steps=total_steps)\n",
    "        sched.step_on_batch = True\n",
    "    elif name == 'cosine-anneal':\n",
    "        sched = CosineAnnealingLR(opt, total_steps)\n",
    "        sched.step_on_batch = True\n",
    "    elif name == 'reduce-on-plateau':\n",
    "        sched= ReduceLROnPlateau(opt)\n",
    "        sched.step_on_batch = False\n",
    "    else:\n",
    "        sched = None\n",
    "    return sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, hparams, metrics=None):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.main_device = 'cpu' if hparams.gpus is None else f'cuda:{hparams.gpus[0]}'\n",
    "        try:\n",
    "            weight = hparams.weight\n",
    "        except AttributeError:\n",
    "            weight = 1.\n",
    "        self.loss = _get_loss(hparams.loss, weight, hparams.reduction, device=self.main_device)\n",
    "        self.bs = hparams.batch_size\n",
    "        self.lr = hparams.lr\n",
    "        self.wd = hparams.wd\n",
    "        self.metrics = ifnone(metrics, [])\n",
    "        model_type = 'normalizer' if isinstance(self, Normalizer) else 'classifier'\n",
    "        self.save_path = hparams.savedir/f'level_{hparams.level}/{model_type}/{hparams.model}'\n",
    "        \n",
    "    def post_init(self):\n",
    "        self.leaf_modules = named_leaf_modules('', self)\n",
    "        self.sizes = get_sizes(self, input_shape=(3, self.hparams.size, self.hparams.size), leaf_modules=self.leaf_modules)\n",
    "        self = self.to(self.main_device)\n",
    "        \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRED\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        lr = self.sched.optimizer.param_groups[-1]['lr']\n",
    "        log = {'train_loss': loss, 'lr': lr}\n",
    "        for metric in self.metrics:\n",
    "            try:\n",
    "                name = metric.__name__\n",
    "            except AttributeError:\n",
    "                name = metric.func.__name__\n",
    "                kws = metric.keywords\n",
    "                for k in kws:\n",
    "                    name += f'_{k}{kws[k]}'\n",
    "            log[name] = metric(y_hat, y)\n",
    "        return {'loss': loss, 'log': log}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        log = {'val_loss': loss}\n",
    "        return {'val_loss': loss, 'log': log}\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        return {'test_loss': self.loss(y_hat, y)}\n",
    "\n",
    "    \n",
    "    def test_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        return {'avg_test_loss': avg_loss}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.sched = _get_scheduler(opt, self.hparams.sched, self.hparams.epochs*len(self.train_dataloader()), self.lr)\n",
    "        return opt\n",
    "    \n",
    "    def on_after_backward(self):\n",
    "        for pg in self.sched.optimizer.param_groups:\n",
    "            for p in pg['params']: p.data.mul_(1 - self.wd*pg['lr'])\n",
    "    \n",
    "    def on_batch_end(self):\n",
    "        if self.sched is not None and self.sched.step_on_batch:\n",
    "            self.sched.step()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.sched is not None and not self.sched.step_on_batch:\n",
    "            self.sched.step()\n",
    "            \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data.train, batch_size=self.bs, shuffle=True)\n",
    "\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return DataLoader(self.data.valid, batch_size=self.bs)\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        # can also return a list of test dataloaders\n",
    "        return DataLoader(self.data.test, batch_size=self.bs) if self.data.test is not None else None\n",
    "    \n",
    "    def load(self, version):\n",
    "        save_dir = self.save_path/f'lightning_logs/version_{version}/checkpoints'\n",
    "        path = next(save_dir.iterdir())\n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        self.load_state_dict(checkpoint['state_dict'])\n",
    "            \n",
    "    def my_summarize(self):\n",
    "        \"\"\"\n",
    "        TODO: change to make it work with `get_size` changes\n",
    "        \"\"\"\n",
    "        summary = pd.DataFrame(self.sizes, columns=['Type', 'Name', 'Output Shape'])\n",
    "        return summary\n",
    "    \n",
    "    def fit(self):\n",
    "        trainer = pl.Trainer(gpus=self.hparams.gpus, default_save_path=self.save_path, min_epochs=self.hparams.epochs, max_epochs=self.hparams.epochs)\n",
    "        self.version = trainer.logger.version\n",
    "        trainer.fit(self)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.eval()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GradesClassifModel(BaseModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(GradesClassifModel, self).__init__(hparams)\n",
    "        tfms = get_transforms(hparams.size)\n",
    "        self.data = (ImageClassifDataset.\n",
    "                     from_folder(Path(hparams.data), lambda x: x.parts[-3], classes=['1', '3'], extensions=['.png'], include=['1', '3'], open_mode='3G').\n",
    "                     split_by_csv(hparams.data_csv).\n",
    "                     to_tensor(tfms=tfms, tfm_y=False))\n",
    "        base_model = timm.create_model(hparams.model, pretrained=not hparams.rand_weights)\n",
    "        self.base_model = nn.Sequential(*list(base_model.children())[:-2])\n",
    "        head = [nn.AdaptiveAvgPool2d(1), nn.Flatten()]\n",
    "        nf = get_num_features(self.base_model)\n",
    "        p = hparams.dropout\n",
    "        head += bn_drop_lin(nf, 512, p=p/2) + bn_drop_lin(512, 2, p=p)\n",
    "        self.head = nn.Sequential(*head)\n",
    "        self.post_init()\n",
    "        self.create_normalizer()\n",
    "        \n",
    "    def create_normalizer(self):\n",
    "        hparams = self.hparams\n",
    "        if hparams.normalizer is not None:\n",
    "            norm = DynamicUnet(hparams.normalizer, n_classes=3, input_shape=(3, hparams.size, hparams.size), pretrained=True)\n",
    "            if hparams.norm_version is not None:\n",
    "                save_dir = self.save_path.parents[1]/'normalizer'/f'{hparams.normalizer}/lightning_logs/version_{hparams.norm_version}/checkpoints'\n",
    "                path = next(save_dir.iterdir())\n",
    "                checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "                state_dict = {}\n",
    "                for k in checkpoint['state_dict']:\n",
    "                    state_dict[k.replace('unet.', '')] = checkpoint['state_dict'][k]\n",
    "                norm.load_state_dict(state_dict)\n",
    "                for p in norm.parameters():\n",
    "                    p.requires_grad = False\n",
    "            norm = norm.to(self.main_device)\n",
    "            self.norm = norm.__call__        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'norm'):\n",
    "            x = self.norm(x)\n",
    "        x = self.base_model(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Normalizer(BaseModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(Normalizer, self).__init__(hparams)\n",
    "        input_shape = (3, hparams.size, hparams.size)\n",
    "        self.unet = DynamicUnet(hparams.normalizer, n_classes=3, input_shape=input_shape, pretrained=not hparams.rand_weights)\n",
    "        # meta = cnn_config(resnet34)\n",
    "        # body = create_body(resnet34, True, None)\n",
    "        # size = (224, 224)\n",
    "        # self.unet = models.unet.DynamicUnet(body, n_classes=3, img_size=size, blur=False, blur_final=True,\n",
    "        #      self_attention=False, y_range=None, norm_type=NormType, last_cross=True,\n",
    "        #      bottle=False)\n",
    "        tfms = get_transforms(hparams.size)\n",
    "        self.data = (NormDataset.\n",
    "                     from_folder(Path(hparams.data), lambda x: x, hparams.csv, extensions=['.png'], include=['1', '3']).\n",
    "                     split_by_csv(hparams.data_csv).\n",
    "                     to_tensor(tfms=tfms))\n",
    "        self.post_init()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.unet(x)\n",
    "    \n",
    "    \n",
    "    def show_results(self, n=16, random=False, imgsize=4, title=None, **kwargs):\n",
    "        n = min(n, self.bs)\n",
    "        fig, axs = plt.subplots(n, 3, figsize=(imgsize*3, imgsize*n))\n",
    "        idxs = np.random.choice(np.arange(len(self.data.valid)), size=n, replace=False)                \n",
    "        inputs = []\n",
    "        targs = []\n",
    "        for idx in idxs:\n",
    "            x, y = self.data.valid[idx]\n",
    "            inputs.append(x)\n",
    "            targs.append(y)            \n",
    "        inputs = torch.stack(inputs).to(next(self.parameters()).device)\n",
    "        preds = self.eval()(inputs).clamp(0, 1)\n",
    "        for ax_r, x, y, z in zip(axs, inputs, targs, preds):\n",
    "            x = x.cpu().numpy().transpose(1, 2, 0)\n",
    "            y = y.numpy().transpose(1, 2, 0)\n",
    "            z = z.detach().cpu().numpy().transpose(1, 2, 0)            \n",
    "            show_img(x, ax=ax_r[0])\n",
    "            show_img(y, ax=ax_r[1])\n",
    "            show_img(z, ax=ax_r[2])\n",
    "        title = ifnone(title, 'input/target/prediction')\n",
    "        fig.suptitle(title)\n",
    "        plt.show()\n",
    "        \n",
    "    def freeze_encoder(self):\n",
    "        for m in self.leaf_modules('', self):\n",
    "            if 'encoder' in m.name and not isinstance(m, nn.BatchNorm2d):\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "    def init_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                with torch.no_grad():\n",
    "                    m.bias.fill_(1e-3)\n",
    "                    m.weight.fill_(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data_read.ipynb.\n",
      "Converted 11_data_loaders.ipynb.\n",
      "Converted 12_data_dataset.ipynb.\n",
      "Converted 13_data_utils.ipynb.\n",
      "Converted 14_data_transforms.ipynb.\n",
      "Converted 20_models_plmodules.ipynb.\n",
      "Converted 21_models_modules.ipynb.\n",
      "Converted 22_models_utils.ipynb.\n",
      "Converted 23_models_hooks.ipynb.\n",
      "Converted 24_models_metrics.ipynb.\n",
      "Converted 80_params_defaults.ipynb.\n",
      "Converted 81_params_norm.ipynb.\n",
      "Converted 82_params_classif.ipynb.\n",
      "Converted 83_params_predict.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
