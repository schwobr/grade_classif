{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.plmodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from grade_classif.data.dataset import ImageClassifDataset, NormDataset\n",
    "from grade_classif.data.transforms import get_transforms\n",
    "from grade_classif.data.utils import show_img\n",
    "from grade_classif.models.utils import named_leaf_modules, get_sizes, get_num_features\n",
    "from grade_classif.models.modules import DynamicUnet, bn_drop_lin, CBR\n",
    "from grade_classif.models.losses import FocalLoss\n",
    "from grade_classif.core import ifnone\n",
    "from grade_classif.imports import *\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.logging import CometLogger\n",
    "from torch.utils.data import DataLoader, RandomSampler, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_loss(loss_name, weight, reduction, device='cpu'):\n",
    "    if loss_name == 'cross-entropy':\n",
    "        loss = nn.CrossEntropyLoss(torch.tensor([weight, 1.], device=device), reduction=reduction)\n",
    "    elif loss_name == 'mse':\n",
    "        loss = nn.MSELoss(reduction=reduction)\n",
    "    elif loss_name == 'focal':\n",
    "        loss = FocalLoss(reduction=reduction)\n",
    "    return loss.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_scheduler(opt, name, total_steps, lr):\n",
    "    if name == 'one-cycle':\n",
    "        sched = OneCycleLR(opt, lr, total_steps=total_steps)\n",
    "        sched.step_on_batch = True\n",
    "    elif name == 'cosine-anneal':\n",
    "        sched = CosineAnnealingLR(opt, total_steps)\n",
    "        sched.step_on_batch = True\n",
    "    elif name == 'reduce-on-plateau':\n",
    "        sched= ReduceLROnPlateau(opt)\n",
    "        sched.step_on_batch = False\n",
    "    else:\n",
    "        sched = None\n",
    "    return sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, hparams, metrics=None):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.main_device = 'cpu' if hparams.gpus is None else f'cuda:{hparams.gpus[0]}'\n",
    "        try:\n",
    "            weight = hparams.weight if hparams.sample_mode == 0 else 1.\n",
    "        except AttributeError:\n",
    "            weight = 1.\n",
    "        self.loss = _get_loss(hparams.loss, weight, hparams.reduction, device=self.main_device)\n",
    "        self.bs = hparams.batch_size\n",
    "        self.lr = hparams.lr\n",
    "        self.wd = hparams.wd\n",
    "        self.metrics = ifnone(metrics, [])\n",
    "        model_type = 'normalizer' if isinstance(self, Normalizer) else 'classifier'\n",
    "        self.save_path = hparams.savedir/f'level_{hparams.level}/{model_type}/{hparams.model}'\n",
    "        \n",
    "    def post_init(self):\n",
    "        self.leaf_modules = named_leaf_modules('', self)\n",
    "        self.sizes, self.leaf_modules = get_sizes(self, input_shape=(3, self.hparams.size, self.hparams.size), leaf_modules=self.leaf_modules)\n",
    "        self = self.to(self.main_device)\n",
    "        \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRED\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        lr = self.sched.optimizer.param_groups[-1]['lr']\n",
    "        log = {'train_loss': loss, 'learning_rate': lr}\n",
    "        return {'loss': loss, 'log': log}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        ret = {'val_loss': loss}\n",
    "        n = y.shape[0]\n",
    "        y_hat = torch.softmax(y_hat, dim=1)\n",
    "        y_hat = y_hat.argmax(dim=-1).view(n,-1)\n",
    "        y = y.view(n,-1)\n",
    "        ret['tp'] = ((y_hat==1)&(y==1)).float().sum()\n",
    "        ret['tn'] = ((y_hat==0)&(y==0)).float().sum()\n",
    "        ret['fp'] = ((y_hat==1)&(y==0)).float().sum()\n",
    "        ret['fn'] = ((y_hat==0)&(y==1)).float().sum()\n",
    "        return ret\n",
    "\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        log = {'val_loss': loss}\n",
    "        tp = torch.stack([x['tp'] for x in outputs]).sum()\n",
    "        fp = torch.stack([x['fp'] for x in outputs]).sum()\n",
    "        tn = torch.stack([x['tn'] for x in outputs]).sum()\n",
    "        fn = torch.stack([x['fn'] for x in outputs]).sum()\n",
    "        for metric in self.metrics:\n",
    "            try:\n",
    "                name = metric.__name__\n",
    "            except AttributeError:\n",
    "                name = metric.func.__name__\n",
    "                kws = metric.keywords\n",
    "                for k in kws:\n",
    "                    name += f'_{k}_{kws[k]}'\n",
    "            log[name] = metric(tp, fp, tn, fn)\n",
    "        return {'val_loss': loss, 'log': log}\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        return {'test_loss': self.loss(y_hat, y)}\n",
    "\n",
    "    \n",
    "    def test_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        return {'avg_test_loss': avg_loss}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.sched = _get_scheduler(opt, self.hparams.sched, self.hparams.epochs*len(self.train_dataloader()), self.lr)\n",
    "        return opt\n",
    "    \n",
    "    def on_after_backward(self):\n",
    "        for pg in self.sched.optimizer.param_groups:\n",
    "            for p in pg['params']: p.data.mul_(1 - self.wd*pg['lr'])\n",
    "    \n",
    "    def on_batch_end(self):\n",
    "        if self.sched is not None and self.sched.step_on_batch:\n",
    "            self.sched.step()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.sched is not None and not self.sched.step_on_batch:\n",
    "            self.sched.step()\n",
    "            \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        sm = self.hparams.sample_mode\n",
    "        w = self.hparams.weight\n",
    "        if sm > 0:\n",
    "            labels = self.data.train.labels == '1'\n",
    "            weights = np.where(labels, w, 1.)\n",
    "            if sm == 1:\n",
    "                sampler = WeightedRandomSampler(weights, 2*len(np.argwhere(~labels)))\n",
    "            else:\n",
    "                sampler = WeightedRandomSampler(weights, 2*len(np.argwhere(labels)), replacement=False)\n",
    "        else:\n",
    "            sampler = RandomSampler(self.data.train)\n",
    "        return DataLoader(self.data.train, batch_size=self.bs, sampler=sampler, drop_last=True)\n",
    "\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return DataLoader(self.data.valid, batch_size=self.bs)\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        # can also return a list of test dataloaders\n",
    "        return DataLoader(self.data.test, batch_size=self.bs) if self.data.test is not None else None\n",
    "    \n",
    "    def load(self, version):\n",
    "        save_dir = self.save_path/f'lightning_logs/version_{version}/checkpoints'\n",
    "        path = next(save_dir.iterdir())\n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        self.load_state_dict(checkpoint['state_dict'])\n",
    "            \n",
    "    def my_summarize(self):\n",
    "        summary = pd.DataFrame({'Name': list(map(lambda x: x.name, self.leaf_modules)), 'Output Shape': self.sizes})\n",
    "        return summary\n",
    "    \n",
    "    def fit(self):\n",
    "        logger = CometLogger(api_key=os.environ['COMET_API_KEY'], workspace='schwobr', save_dir=self.save_path, project_name='grade-classif')\n",
    "        trainer = pl.Trainer(gpus=self.hparams.gpus, default_save_path=self.save_path, logger=logger, min_epochs=self.hparams.epochs, max_epochs=self.hparams.epochs)\n",
    "        self.version = trainer.logger.version\n",
    "        trainer.fit(self)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.eval()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GradesClassifModel(BaseModule):\n",
    "    def __init__(self, hparams, **kwargs):\n",
    "        super().__init__(hparams, **kwargs)\n",
    "        tfms = get_transforms(hparams.size)\n",
    "        if hparams.concepts is not None and hparams.concept_classes is not None:\n",
    "            conc_classes_df = pd.read_csv(hparams.concept_classes, index_col=0)\n",
    "            if hparams.filt != 'all':\n",
    "                ok = conc_classes_df.loc[conc_classes_df['type'] == hparams.filt].index.values\n",
    "            else:\n",
    "                ok = conc_classes_df.loc[conc_classes_df['type'] != 'garb'].index.values\n",
    "            conc_df = pd.read_csv(hparams.concepts, index_col='patchId')\n",
    "            def filt(x):\n",
    "                return conc_df.loc[x.stem, 'concept'] in ok\n",
    "        else:\n",
    "            filt = None\n",
    "        self.data = (ImageClassifDataset.\n",
    "                     from_folder(Path(hparams.data), lambda x: x.parts[-3], classes=['1', '3'], extensions=['.png'], include=['1', '3'], open_mode='3G', filterfunc=filt).\n",
    "                     split_by_csv(hparams.data_csv).\n",
    "                     to_tensor(tfms=tfms, tfm_y=False))\n",
    "        weight = (self.data.train.labels == '3').sum()/(self.data.train.labels == '1').sum() if hparams.sample_mode == 0 else 1\n",
    "        self.hparams.weight = weight\n",
    "        self.loss = _get_loss(hparams.loss, weight, hparams.reduction, device=self.main_device)\n",
    "        if 'cbr' in hparams.model:\n",
    "            args = map(int, hparams.model.split('_')[1:])\n",
    "            base_model = CBR(*args)\n",
    "            cut = -3\n",
    "        else:\n",
    "            base_model = timm.create_model(hparams.model, pretrained=not hparams.rand_weights)\n",
    "            cut = -2\n",
    "        self.base_model = nn.Sequential(*list(base_model.children())[:cut])\n",
    "        head = [nn.AdaptiveAvgPool2d(1), nn.Flatten()]\n",
    "        nf = get_num_features(self.base_model)\n",
    "        p = hparams.dropout\n",
    "        head += bn_drop_lin(nf, nf, p=p/2) + bn_drop_lin(nf, 2, p=p)\n",
    "        self.head = nn.Sequential(*head)\n",
    "        self.post_init()\n",
    "        self.create_normalizer()\n",
    "        \n",
    "    def create_normalizer(self):\n",
    "        hparams = self.hparams\n",
    "        if hparams.normalizer is not None:\n",
    "            norm = DynamicUnet(hparams.normalizer, n_classes=3, input_shape=(3, hparams.size, hparams.size), pretrained=True)\n",
    "            if hparams.norm_version is not None:\n",
    "                save_dir = self.save_path.parents[1]/'normalizer'/f'{hparams.normalizer}/lightning_logs/version_{hparams.norm_version}/checkpoints'\n",
    "                path = next(save_dir.iterdir())\n",
    "                checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "                state_dict = {}\n",
    "                for k in checkpoint['state_dict']:\n",
    "                    state_dict[k.replace('unet.', '')] = checkpoint['state_dict'][k]\n",
    "                norm.load_state_dict(state_dict)\n",
    "                for p in norm.parameters():\n",
    "                    p.requires_grad = False\n",
    "            norm = norm.to(self.main_device)\n",
    "            self.norm = norm.__call__        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'norm'):\n",
    "            x = self.norm(x)\n",
    "        x = self.base_model(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        pred = super().predict(x)\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Normalizer(BaseModule):\n",
    "    def __init__(self, hparams, **kwargs):\n",
    "        super().__init__(hparams, **kwargs)\n",
    "        input_shape = (3, hparams.size, hparams.size)\n",
    "        self.unet = DynamicUnet(hparams.normalizer, n_classes=3, input_shape=input_shape, pretrained=not hparams.rand_weights)\n",
    "        # meta = cnn_config(resnet34)\n",
    "        # body = create_body(resnet34, True, None)\n",
    "        # size = (224, 224)\n",
    "        # self.unet = models.unet.DynamicUnet(body, n_classes=3, img_size=size, blur=False, blur_final=True,\n",
    "        #      self_attention=False, y_range=None, norm_type=NormType, last_cross=True,\n",
    "        #      bottle=False)\n",
    "        tfms = get_transforms(hparams.size)\n",
    "        self.data = (NormDataset.\n",
    "                     from_folder(Path(hparams.data), hparams.csv, extensions=['.png'], include=['1', '3']).\n",
    "                     split_by_csv(hparams.data_csv).\n",
    "                     to_tensor(tfms=tfms))\n",
    "        self.post_init()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.unet(x)\n",
    "    \n",
    "    \n",
    "    def show_results(self, n=16, random=False, imgsize=4, title=None, **kwargs):\n",
    "        n = min(n, self.bs)\n",
    "        fig, axs = plt.subplots(n, 3, figsize=(imgsize*3, imgsize*n))\n",
    "        idxs = np.random.choice(np.arange(len(self.data.valid)), size=n, replace=False)                \n",
    "        inputs = []\n",
    "        targs = []\n",
    "        for idx in idxs:\n",
    "            x, y = self.data.valid[idx]\n",
    "            inputs.append(x)\n",
    "            targs.append(y)            \n",
    "        inputs = torch.stack(inputs).to(next(self.parameters()).device)\n",
    "        preds = self.eval()(inputs).clamp(0, 1)\n",
    "        for ax_r, x, y, z in zip(axs, inputs, targs, preds):\n",
    "            x = x.cpu().numpy().transpose(1, 2, 0)\n",
    "            y = y.numpy().transpose(1, 2, 0)\n",
    "            z = z.detach().cpu().numpy().transpose(1, 2, 0)            \n",
    "            show_img(x, ax=ax_r[0])\n",
    "            show_img(y, ax=ax_r[1])\n",
    "            show_img(z, ax=ax_r[2])\n",
    "        title = ifnone(title, 'input/target/prediction')\n",
    "        fig.suptitle(title)\n",
    "        plt.show()\n",
    "        \n",
    "    def freeze_encoder(self):\n",
    "        for m in self.leaf_modules('', self):\n",
    "            if 'encoder' in m.name and not isinstance(m, nn.BatchNorm2d):\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "    def init_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                with torch.no_grad():\n",
    "                    m.bias.fill_(1e-3)\n",
    "                    m.weight.fill_(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grade_classif.params.parser import hparams\n",
    "hparams.model = 'cbr_5_32_4'\n",
    "model = GradesClassifModel(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "         Name        Type Params\n",
      "0  base_model  Sequential    1 M\n",
      "1        head  Sequential   67 K\n"
     ]
    }
   ],
   "source": [
    "model.summarize('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Output Shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_model.0.conv</td>\n",
       "      <td>(2, 32, 112, 112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base_model.0.bn</td>\n",
       "      <td>(2, 32, 112, 112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>base_model.0.relu</td>\n",
       "      <td>(2, 32, 112, 112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>base_model.1.conv</td>\n",
       "      <td>(2, 64, 56, 56)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>base_model.1.bn</td>\n",
       "      <td>(2, 64, 56, 56)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>base_model.1.relu</td>\n",
       "      <td>(2, 64, 56, 56)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>base_model.2.conv</td>\n",
       "      <td>(2, 128, 28, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>base_model.2.bn</td>\n",
       "      <td>(2, 128, 28, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>base_model.2.relu</td>\n",
       "      <td>(2, 128, 28, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>base_model.3.conv</td>\n",
       "      <td>(2, 256, 14, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>base_model.3.bn</td>\n",
       "      <td>(2, 256, 14, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>base_model.3.relu</td>\n",
       "      <td>(2, 256, 14, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>head.0</td>\n",
       "      <td>(2, 256, 1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>head.1</td>\n",
       "      <td>(2, 256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>head.2</td>\n",
       "      <td>(2, 256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>head.3</td>\n",
       "      <td>(2, 256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>head.4</td>\n",
       "      <td>(2, 256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>head.5</td>\n",
       "      <td>(2, 256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>head.6</td>\n",
       "      <td>(2, 256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>head.7</td>\n",
       "      <td>(2, 2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name       Output Shape\n",
       "0   base_model.0.conv  (2, 32, 112, 112)\n",
       "1     base_model.0.bn  (2, 32, 112, 112)\n",
       "2   base_model.0.relu  (2, 32, 112, 112)\n",
       "3   base_model.1.conv    (2, 64, 56, 56)\n",
       "4     base_model.1.bn    (2, 64, 56, 56)\n",
       "5   base_model.1.relu    (2, 64, 56, 56)\n",
       "6   base_model.2.conv   (2, 128, 28, 28)\n",
       "7     base_model.2.bn   (2, 128, 28, 28)\n",
       "8   base_model.2.relu   (2, 128, 28, 28)\n",
       "9   base_model.3.conv   (2, 256, 14, 14)\n",
       "10    base_model.3.bn   (2, 256, 14, 14)\n",
       "11  base_model.3.relu   (2, 256, 14, 14)\n",
       "12             head.0     (2, 256, 1, 1)\n",
       "13             head.1           (2, 256)\n",
       "14             head.2           (2, 256)\n",
       "15             head.3           (2, 256)\n",
       "16             head.4           (2, 256)\n",
       "17             head.5           (2, 256)\n",
       "18             head.6           (2, 256)\n",
       "19             head.7             (2, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.my_summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(model.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.cuda()\n",
    "y = y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)\n",
    "loss = nn.CrossEntropyLoss()(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data.read.ipynb.\n",
      "Converted 11_data.loaders.ipynb.\n",
      "Converted 12_data.dataset.ipynb.\n",
      "Converted 13_data.utils.ipynb.\n",
      "Converted 14_data.transforms.ipynb.\n",
      "Converted 20_models.plmodules.ipynb.\n",
      "Converted 21_models.modules.ipynb.\n",
      "Converted 22_models.utils.ipynb.\n",
      "Converted 23_models.hooks.ipynb.\n",
      "Converted 24_models.metrics.ipynb.\n",
      "Converted 25_models.losses.ipynb.\n",
      "Converted 80_params.defaults.ipynb.\n",
      "Converted 81_params.parser.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
