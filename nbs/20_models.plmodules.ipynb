{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.plmodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PytorchLightning Modules\n",
    "> Modules implementing <a href=\\\"https://pytorch-lightning.readthedocs.io/en/0.6.0/lightning-module.html\\\">LightningModule</a> interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pytorch_lightning as pl\n",
    "from kornia.color import rgb_to_grayscale, rgb_to_hsv\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, RandomSampler, WeightedRandomSampler, Dataset\n",
    "from torch.optim import Optimizer\n",
    "from grade_classif.core import ifnone\n",
    "from grade_classif.data.color import rgb_to_e, rgb_to_h, rgb_to_heg, rgb_to_lab\n",
    "from grade_classif.data.dataset import ImageClassifDataset, NormDataset\n",
    "from grade_classif.data.transforms import *\n",
    "from grade_classif.data.utils import show_img\n",
    "from grade_classif.imports import *\n",
    "from grade_classif.models.losses import BCE, FocalLoss\n",
    "from grade_classif.models.metrics import pcc, ssim\n",
    "from grade_classif.models.modules import *\n",
    "from grade_classif.models.utils import (\n",
    "    gaussian_mask,\n",
    "    get_num_features,\n",
    "    get_sizes,\n",
    "    named_leaf_modules,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "_open_functions = {\n",
    "    \"LAB\": rgb_to_lab,\n",
    "    \"HSV\": rgb_to_hsv,\n",
    "    \"3G\": rgb_to_grayscale,\n",
    "    \"H\": rgb_to_h,\n",
    "    \"E\": rgb_to_e,\n",
    "    \"HEG\": rgb_to_heg,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_loss(\n",
    "    loss_name: str, weight: float, reduction: str, device: str = \"cpu\", nc: int = 2\n",
    ") -> Callable[[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "    if loss_name == \"cross-entropy\":\n",
    "        loss = nn.CrossEntropyLoss(\n",
    "            torch.tensor([weight] + [1.0] * (nc - 1), device=device),\n",
    "            reduction=reduction,\n",
    "        )\n",
    "    elif loss_name == \"bce\":\n",
    "        loss = BCE(\n",
    "            reduction=reduction, pos_weight=torch.tensor([1 / weight], device=device)\n",
    "        )\n",
    "    elif loss_name == \"mse\":\n",
    "        loss = nn.MSELoss(reduction=reduction)\n",
    "    elif loss_name == \"focal\":\n",
    "        loss = FocalLoss(reduction=reduction)\n",
    "    return loss.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_scheduler(\n",
    "    opt: Optimizer, name: str, total_steps: int, lr: float\n",
    ") -> torch.optim.lr_scheduler._LRScheduler:\n",
    "    if name == \"one-cycle\":\n",
    "        sched = OneCycleLR(opt, lr, total_steps=total_steps)\n",
    "        interval = \"step\"\n",
    "    elif name == \"cosine-anneal\":\n",
    "        sched = CosineAnnealingLR(opt, total_steps)\n",
    "        interval = \"step\"\n",
    "    elif name == \"reduce-on-plateau\":\n",
    "        sched = ReduceLROnPlateau(opt)\n",
    "        interval = \"epoch\"\n",
    "    else:\n",
    "        return None\n",
    "    return {\"scheduler\": sched, \"interval\": interval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datafolder: Path,\n",
    "        data_csv: Path,\n",
    "        batch_size: int = 32,\n",
    "        size: int = 299,\n",
    "        transforms: Optional[int] = None,\n",
    "        patch_classes: Optional[Path] = None,\n",
    "        concepts: Optional[Path] = None,\n",
    "        concept_classes: Optional[Path] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.datafolder = datafolder\n",
    "        self.data_csv = data_csv\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_classes = None\n",
    "        self.concepts = None\n",
    "        self.concept_classes = None\n",
    "        self.size = 299\n",
    "        self.transforms = transforms\n",
    "        if transforms is not None:\n",
    "            tfm_func = globals()[f\"get_transforms{transforms}\"]\n",
    "            self.tfms = tfm_func(size)\n",
    "        else:\n",
    "            self.tfms = []\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return DataLoader(\n",
    "            self.data.valid, batch_size=self.batch_size, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        # OPTIONAL\n",
    "        # can also return a list of test dataloaders\n",
    "        if self.data.test is not None:\n",
    "            return DataLoader(self.data.test, batch_size=self.batch_size)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_filt(self) -> Optional[Callable[[Path], bool]]:\n",
    "        if self.patch_classes is not None:\n",
    "            patch_classes_df = pd.read_csv(self.patch_classes, index_col=\"patchId\")\n",
    "            x_type = patch_classes_df.loc[x.stem, \"type\"]\n",
    "            if self.filt != \"all\":\n",
    "\n",
    "                def filt(x):\n",
    "                    return x_type == self.filt\n",
    "\n",
    "            else:\n",
    "\n",
    "                def filt(x):\n",
    "                    return x_type != \"garb\"\n",
    "\n",
    "        elif self.concepts is not None and self.concept_classes is not None:\n",
    "            conc_classes_df = pd.read_csv(self.concept_classes, index_col=0)\n",
    "            if self.filt != \"all\":\n",
    "                ok = conc_classes_df.loc[\n",
    "                    conc_classes_df[\"type\"] == self.filt\n",
    "                ].index.values\n",
    "            else:\n",
    "                ok = conc_classes_df.loc[conc_classes_df[\"type\"] != \"garb\"].index.values\n",
    "            conc_df = pd.read_csv(self.concepts, index_col=\"patchId\")\n",
    "\n",
    "            def filt(x):\n",
    "                return conc_df.loc[x.stem, \"concept\"] in ok\n",
    "\n",
    "        else:\n",
    "            filt = None\n",
    "        return filt\n",
    "\n",
    "    def show_some(self, n: int = 8, split: str = \"train\", imgsize: int = 4):\n",
    "        fig, axs = plt.subplots(n, 2, figsize=(imgsize * 2, imgsize * n))\n",
    "        data = getattr(self.data, split)\n",
    "        idxs = np.random.choice(np.arange(len(data)), size=n, replace=False)\n",
    "        for ax_r, idx in zip(axs, idxs):\n",
    "            x, x_tfmed = data.get_orig_tfmed(idx)\n",
    "            show_img(x, ax=ax_r[0])\n",
    "            show_img(x_tfmed, ax=ax_r[1])\n",
    "        title = \"original/transformed\"\n",
    "        fig.suptitle(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseModule(pl.LightningModule):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 299,\n",
    "        gpus: Optional[List[int]] = None,\n",
    "        model: Optional[str] = None,\n",
    "        normalizer: Optional[str] = None,\n",
    "        metrics: Optional[List[Callable[..., Number]]] = None,\n",
    "        savedir: Optional[Union[Path, str]] = None,\n",
    "        level: Optional[int] = None,\n",
    "        resume: Optional[str] = None,\n",
    "        sample_mode: int = 0,\n",
    "        weight: float = 1,\n",
    "        epochs: int = 5,\n",
    "        loss: str = \"mse\",\n",
    "        reduction: str = \"mean\",\n",
    "        lr: float = 1e-3,\n",
    "        wd: float = 0.01,\n",
    "        train_percent: float = 1.0,\n",
    "        sched: Optional[str] = None,\n",
    "        rand_weights: bool = False,\n",
    "        dropout: float = 0.5,\n",
    "        version: Optional[str] = None,\n",
    "        norm_version: Optional[str] = None,\n",
    "        open_mode: str = \"RGB\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if model is None and normalizer is None:\n",
    "            raise AttributeError(\n",
    "                \"at least one of model and normalizer should ne specified\"\n",
    "            )\n",
    "        self.main_device = \"cpu\" if gpus is None else f\"cuda:{gpus[0]}\"\n",
    "        # self.main_device = 'cuda:1'\n",
    "        self.metrics = ifnone(metrics, [])\n",
    "        model_type = \"normalizer\" if isinstance(self, Normalizer) else \"classifier\"\n",
    "        model_name = model if model_type == \"classifier\" else normalizer\n",
    "        savedir = ifnone(Path(savedir), Path.cwd() / \"log\")\n",
    "        level = ifnone(level, -1)\n",
    "        self.save_path = Path(savedir) / f\"level_{level}\"\n",
    "        self.save_path = self.save_path / f\"{model_type}/{model_name}\"\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def post_init(self):\n",
    "        leaf_modules = named_leaf_modules(self)\n",
    "        size = self.hparams.size\n",
    "        self.sizes, self.leaf_modules = get_sizes(\n",
    "            self, input_shape=(3, size, size), leaf_modules=leaf_modules\n",
    "        )\n",
    "        self = self.to(self.main_device)\n",
    "        if self.hparams.resume is not None:\n",
    "            self.load(self.hparams.resume)\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.train()\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> torch.Tensor:\n",
    "        # REQUIRED\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        lr = self.sched.optimizer.param_groups[-1][\"lr\"]\n",
    "        log = {\"train_loss\": loss, \"learning_rate\": lr}\n",
    "        self.log_dict(log, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> torch.Tensor:\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> torch.Tensor:\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> Union[Optimizer, Dict[str, Any]]:\n",
    "        # REQUIRED\n",
    "        hparams = self.hparams\n",
    "        try:\n",
    "            weight = hparams.weight if hparams.sample_mode == 0 else 1.0\n",
    "        except AttributeError:\n",
    "            weight = 1.0\n",
    "        self.loss = _get_loss(\n",
    "            hparams.loss, weight, hparams.reduction, device=self.main_device\n",
    "        )\n",
    "        self.lr = hparams.lr\n",
    "        self.wd = hparams.wd\n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        n_train_dl = len(self.trainer.datamodule.train_dataloader())\n",
    "        n_iter = int(hparams.train_percent * hparams.epochs * n_train_dl)\n",
    "        sched = _get_scheduler(self.opt, hparams.sched, n_iter, self.lr)\n",
    "        if sched is None:\n",
    "            return self.opt\n",
    "        else:\n",
    "            self.sched = sched[\"scheduler\"]\n",
    "            return {\"optimizer\": self.opt, \"lr_scheduler\": sched}\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        for pg in self.opt.param_groups:\n",
    "            for p in pg[\"params\"]:\n",
    "                p.data.mul_(1 - self.wd * pg[\"lr\"])\n",
    "\n",
    "    def load(self, version: str, ckpt_epoch: int = None):\n",
    "        \"\"\"\n",
    "        Load a specific `version` of current model, stored in\n",
    "        `self.save_path/lightning_logs`. If multiple checkpoints have been\n",
    "        stored, `ckpt_epoch` can be specified to load a specific epoch. Else\n",
    "        the latest epoch is loaded.\n",
    "        \"\"\"\n",
    "        save_dir = self.save_path / f\"lightning_logs/version_{version}/checkpoints\"\n",
    "        path = (\n",
    "            list(save_dir.iterdir())[-1]\n",
    "            if ckpt_epoch is None\n",
    "            else save_dir / f\"_ckpt_epoch_{ckpt_epoch}.ckpt\"\n",
    "        )\n",
    "        checkpoint = torch.load(path, map_location=lambda stor, loc: stor)\n",
    "        self.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    def my_summarize(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame containing the list of all leaf modules of current\n",
    "        model, with their corresponding output shape.\n",
    "        \"\"\"\n",
    "        named_modules = list(map(lambda x: x.name, self.leaf_modules))\n",
    "        summary = pd.DataFrame({\"Name\": named_modules, \"Output Shape\": self.sizes})\n",
    "        return summary\n",
    "\n",
    "    def fit(self, dm: pl.LightningDataModule, **kwargs):\n",
    "        \"\"\"\n",
    "        Fit the model using parameters stored in `hparams`.\n",
    "        \"\"\"\n",
    "        logger = CometLogger(\n",
    "            api_key=os.environ[\"COMET_API_KEY\"],\n",
    "            workspace=\"schwobr\",\n",
    "            save_dir=self.save_path,\n",
    "            project_name=\"grade-classif\",\n",
    "        )\n",
    "        logger.experiment.add_tag(\"norm\" if isinstance(self, Normalizer) else \"classif\")\n",
    "        ckpt_path = (\n",
    "            self.save_path\n",
    "            / \"lightning_logs\"\n",
    "            / f\"version_{logger.version}\"\n",
    "            / \"checkpoints\"\n",
    "        )\n",
    "        ckpt_callback = ModelCheckpoint(ckpt_path, save_top_k=3)\n",
    "        trainer = pl.Trainer(\n",
    "            gpus=self.hparams.gpus,\n",
    "            checkpoint_callback=ckpt_callback,\n",
    "            logger=logger,\n",
    "            min_epochs=self.hparams.epochs,\n",
    "            max_epochs=self.hparams.epochs,\n",
    "            train_percent_check=self.hparams.train_percent,\n",
    "            val_percent_check=self.hparams.train_percent,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.version = trainer.logger.version\n",
    "        trainer.fit(self, dm)\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Make a prediction on batch `x`.\n",
    "        \"\"\"\n",
    "        return self.eval()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base class for all classes implementing [`LightningModule`](https://pytorch-lightning.readthedocs.io/en/0.6.0/lightning-module.html) interface. Defines a lot of convenience function to be used by children classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseModule.load\" class=\"doc_header\"><code>BaseModule.load</code><a href=\"__main__.py#L92\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseModule.load</code>(**`version`**, **`ckpt_epoch`**=*`None`*)\n",
       "\n",
       "Load a specific `version` of current model, stored in \n",
       "`self.save_path/lightning_logs`. If multiple checkpoints have been\n",
       "stored, `ckpt_epoch` can be specified to load a specific epoch. Else\n",
       "the latest epoch is loaded."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseModule.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseModule.my_summarize\" class=\"doc_header\"><code>BaseModule.my_summarize</code><a href=\"__main__.py#L106\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseModule.my_summarize</code>()\n",
       "\n",
       "Get a DataFrame containing the list of all leaf modules of current\n",
       "model, with their corresponding output shape."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseModule.my_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseModule.fit\" class=\"doc_header\"><code>BaseModule.fit</code><a href=\"__main__.py#L116\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseModule.fit</code>(**`dm`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Fit the model using parameters stored in [`hparams`](params.parser.html#hparams)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseModule.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BaseModule.predict\" class=\"doc_header\"><code>BaseModule.predict</code><a href=\"__main__.py#L139\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BaseModule.predict</code>(**`x`**)\n",
       "\n",
       "Make a prediction on batch `x`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BaseModule.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class NormDataModule(BaseDataModule):\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        data = NormDataset.from_folder(\n",
    "            self.datafolder, extensions=[\".png\"], filterfunc=self.get_filt()\n",
    "        ).split_by_csv(self.data_csv)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            tfm_func = globals()[f\"get_transforms{self.transforms}\"]\n",
    "            self.tfms = tfm_func(self.size, num_els=len(data.valid))\n",
    "\n",
    "        self.data = data.to_tensor(tfms=self.tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_keys_in_list_and_apply(\n",
    "    list_of_dicts: Sequence[Dict[str, Any]],\n",
    "    *keys,\n",
    "    apply_func: Optional[Callable[[Any], Any]] = None\n",
    ") -> List[List[Any]]:\n",
    "    res = []\n",
    "    for k in keys:\n",
    "        sub_list = []\n",
    "        for d in list_of_dicts:\n",
    "            sub_list.append(d[k])\n",
    "        if apply_func is not None:\n",
    "            sub_list = apply_func(sub_list)\n",
    "        res.append(sub_list)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Normalizer(BaseModule):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        input_shape = (3, self.hparams.size, self.hparams.size)\n",
    "        self.unet = DynamicUnet(\n",
    "            self.hparams.normalizer,\n",
    "            n_classes=3,\n",
    "            input_shape=input_shape,\n",
    "            pretrained=not self.hparams.rand_weights,\n",
    "        )\n",
    "        self.post_init()\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        for tfm in self.trainer.datamodule.data.valid.tfms:\n",
    "            if \"Deterministic\" in str(type(tfm)):\n",
    "                tfm.n = 0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = _open_functions[self.hparams.open_mode](x).detach()\n",
    "        return self.unet(x)\n",
    "\n",
    "    def show_results(\n",
    "        self, ds: Dataset, n: int = 16, imgsize: int = 4, title: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot `n` predictions from the normalizer using samples from dataset\n",
    "        `ds`. Each line will contain input, target and prediction images (in\n",
    "        that order).\n",
    "        \"\"\"\n",
    "        n = min(n, self.bs)\n",
    "        fig, axs = plt.subplots(n, 3, figsize=(imgsize * 3, imgsize * n))\n",
    "        idxs = np.random.choice(np.arange(len(ds)), size=n, replace=False)\n",
    "        inputs = []\n",
    "        targs = []\n",
    "        for idx in idxs:\n",
    "            x, y = ds[idx]\n",
    "            inputs.append(x)\n",
    "            targs.append(y)\n",
    "        inputs = torch.stack(inputs).to(next(self.main_device))\n",
    "        preds = self.predict(inputs).clamp(0, 1)\n",
    "        for ax_r, x, y, z in zip(axs, inputs, targs, preds):\n",
    "            x = x.cpu().numpy().transpose(1, 2, 0)\n",
    "            y = y.numpy().transpose(1, 2, 0)\n",
    "            z = z.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "            show_img(x, ax=ax_r[0])\n",
    "            show_img(y, ax=ax_r[1])\n",
    "            show_img(z, ax=ax_r[2])\n",
    "        title = ifnone(title, \"input/target/prediction\")\n",
    "        fig.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        \"\"\"\n",
    "        Freeze the encoder part of the normalizer.\n",
    "        \"\"\"\n",
    "        for m in self.leaf_modules:\n",
    "            if \"encoder\" in m.name and not isinstance(m, nn.BatchNorm2d):\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def init_bn(self):\n",
    "        \"\"\"\n",
    "        Initialize BatchNorm layers with bias `1e-3` and weights `1`.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                with torch.no_grad():\n",
    "                    m.bias.fill_(1e-3)\n",
    "                    m.weight.fill_(1.0)\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self.predict(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        ret = {\"loss\": loss}\n",
    "        bs = y.shape[0]\n",
    "        y = rgb_to_lab(y.detach())\n",
    "        y_hat = rgb_to_lab(y_hat.detach())\n",
    "        ret[\"mu_x\"] = torch.mean(y[:, 0], axis=(1, 2))\n",
    "        ret[\"sigma_x\"] = torch.std(y[:, 0], axis=(1, 2))\n",
    "        ret[\"mu_y\"] = torch.mean(y_hat[:, 0], axis=(1, 2))\n",
    "        ret[\"sigma_y\"] = torch.std(y_hat[:, 0], axis=(1, 2))\n",
    "        ret[\"mu_xy\"] = torch.mean(y[:, 0] * y_hat[:, 0], axis=(1, 2))\n",
    "        return ret\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Dict[str, torch.Tensor]]):\n",
    "        # OPTIONAL\n",
    "        log = {}\n",
    "        mu_x, sigma_x, mu_y, sigma_y, mu_xy = get_keys_in_list_and_apply(\n",
    "            outputs, \"mu_x\", \"sigma_x\", \"mu_y\", \"sigma_y\", \"mu_xy\", apply_func=torch.cat\n",
    "        )\n",
    "        m_ssim = ssim(mu_x, sigma_x, mu_y, sigma_y, mu_xy)\n",
    "        m_pcc = pcc(mu_x, sigma_x, mu_y, sigma_y, mu_xy)\n",
    "        m_cd = sigma_y / mu_y - sigma_x / mu_x\n",
    "        log[\"ssim\"] = m_ssim.mean()\n",
    "        log[\"pcc\"] = m_pcc.mean()\n",
    "        log[\"cd\"] = m_cd.mean()\n",
    "        log[\"val_loss\"] = torch.mean([output[\"loss\"] for output in outputs])\n",
    "        self.log_dict(log)\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        return self.validation_step(batch, batch_nb)\n",
    "\n",
    "    def test_epoch_end(self, outputs: List[Dict[str, torch.tensor]]):\n",
    "        return self.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module containing a UNet normalizer. Takes grayscale images as inputs and their corresponding colored version as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# FULLY DEPRECATED FOR NOW\n",
    "class NormalizerAN(BaseModule):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, hparams, **kwargs):\n",
    "        super().__init__(hparams, **kwargs)\n",
    "        input_shape = (3, hparams.size, hparams.size)\n",
    "        self.unet = DynamicUnet(\n",
    "            hparams.normalizer,\n",
    "            n_classes=3,\n",
    "            input_shape=input_shape,\n",
    "            pretrained=not hparams.rand_weights,\n",
    "        )\n",
    "\n",
    "        if \"cbr\" in hparams.discriminator:\n",
    "            args = map(int, hparams.discriminator.split(\"_\")[1:])\n",
    "            base_model = CBR(*args)\n",
    "            cut = -3\n",
    "        elif \"sasa\" in hparams.discriminator:\n",
    "            args = map(int, hparams.discriminator.split(\"_\")[1:])\n",
    "            base_model = SASA(*args)\n",
    "            cut = -3\n",
    "        elif \"sanet\" in hparams.discriminator:\n",
    "            splits = hparams.discriminator.split(\"_\")\n",
    "            kernel_size = int(splits[-1])\n",
    "            base_model = globals()[splits[0]](kernel_size)\n",
    "            cut = -2\n",
    "        else:\n",
    "            base_model = timm.create_model(\n",
    "                hparams.discriminator, pretrained=not hparams.rand_weights\n",
    "            )\n",
    "            cut = -2\n",
    "        base_model = nn.Sequential(*list(base_model.children())[:cut])\n",
    "        head = [nn.AdaptiveAvgPool2d(1), nn.Flatten()]\n",
    "        nf = get_num_features(base_model)\n",
    "        p = hparams.dropout\n",
    "        nc = 3\n",
    "        head += bn_drop_lin(nf, nf, p=p / 2) + bn_drop_lin(nf, nc, p=p)\n",
    "        head = nn.Sequential(*head)\n",
    "        self.discriminator = nn.Sequential(base_model, head)\n",
    "        self.loss = _get_loss(\n",
    "            hparams.loss, 1, hparams.reduction, device=self.main_device, nc=3\n",
    "        )\n",
    "\n",
    "        if hparams.norm_csv is not None:\n",
    "            df = pd.read_csv(hparams.norm_csv, index_col=\"scan\")\n",
    "\n",
    "            def filt1(x):\n",
    "                return df.loc[x.parent.name, \"category\"] == 1\n",
    "\n",
    "        else:\n",
    "            filt1 = None\n",
    "\n",
    "        if hparams.patch_classes is not None:\n",
    "            patch_classes_df = pd.read_csv(hparams.patch_classes, index_col=\"patchId\")\n",
    "            if hparams.filt != \"all\":\n",
    "\n",
    "                def filt2(x):\n",
    "                    return patch_classes_df.loc[x.stem, \"type\"] == hparams.filt\n",
    "\n",
    "            else:\n",
    "\n",
    "                def filt2(x):\n",
    "                    return patch_classes_df.loc[x.stem, \"type\"] != \"garb\"\n",
    "\n",
    "        elif hparams.concepts is not None and hparams.concept_classes is not None:\n",
    "            conc_classes_df = pd.read_csv(hparams.concept_classes, index_col=0)\n",
    "            if hparams.filt2 != \"all\":\n",
    "                ok = conc_classes_df.loc[\n",
    "                    conc_classes_df[\"type\"] == hparams.filt\n",
    "                ].index.values\n",
    "            else:\n",
    "                ok = conc_classes_df.loc[conc_classes_df[\"type\"] != \"garb\"].index.values\n",
    "            conc_df = pd.read_csv(hparams.concepts, index_col=\"patchId\")\n",
    "\n",
    "            def filt2(x):\n",
    "                return conc_df.loc[x.stem, \"concept\"] in ok\n",
    "\n",
    "        else:\n",
    "            filt2 = None\n",
    "\n",
    "        if filt1 is None:\n",
    "            filt = filt2\n",
    "        else:\n",
    "            if filt2 is None:\n",
    "                filt = filt1\n",
    "            else:\n",
    "\n",
    "                def filt(x):\n",
    "                    return filt1(x) and filt2(x)\n",
    "\n",
    "        def label_func(x):\n",
    "            if \"PACS04\" in x.name:\n",
    "                return \"04\"\n",
    "            elif \"PACS05\" in x.name:\n",
    "                return \"05\"\n",
    "            else:\n",
    "                return \"08\"\n",
    "\n",
    "        data = ImageClassifDataset.from_folder(\n",
    "            hparams.data,\n",
    "            label_func,\n",
    "            classes=[\"04\", \"05\", \"08\"],\n",
    "            extensions=[\".png\"],\n",
    "            open_mode=hparams.open_mode,\n",
    "            filterfunc=filt,\n",
    "        ).split_by_csv(hparams.data_csv)\n",
    "\n",
    "        n = len(data.train)\n",
    "        weights = [\n",
    "            np.float32(n / (data.train.labels == \"04\").sum()),\n",
    "            np.float32(n / (data.train.labels == \"05\").sum()),\n",
    "            np.float32(n / (data.train.labels == \"08\").sum()),\n",
    "        ]\n",
    "        self.hparams.weights = weights\n",
    "\n",
    "        if hparams.transforms:\n",
    "            tfms = globals()[f\"get_transforms{hparams.transforms}\"](\n",
    "                hparams.size, num_els=len(data.valid)\n",
    "            )\n",
    "        else:\n",
    "            tfms = []\n",
    "\n",
    "        if hparams.geometric_loss is not None:\n",
    "            self.geometric_loss = _get_loss(\n",
    "                hparams.geometric_loss, 1, hparams.reduction, device=self.main_device\n",
    "            )\n",
    "        else:\n",
    "            self.geometric_loss = lambda x: 0\n",
    "\n",
    "        self.data = data.to_tensor(tfms=tfms, tfm_y=False)\n",
    "\n",
    "        self.leaf_modules = named_leaf_modules(self.unet)\n",
    "        self.sizes, self.leaf_modules = get_sizes(\n",
    "            self.unet,\n",
    "            input_shape=(3, self.hparams.size, self.hparams.size),\n",
    "            leaf_modules=self.leaf_modules,\n",
    "        )\n",
    "        self = self.to(self.main_device)\n",
    "        if self.hparams.resume is not None:\n",
    "            self.load(self.hparams.resume)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unet(x)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        sm = self.hparams.sample_mode\n",
    "        w = self.hparams.weights\n",
    "        labels = self.data.train.labels\n",
    "        if sm > 0:\n",
    "            weights = np.where(labels == \"04\", w[0], 1.0)\n",
    "            weights[labels == \"05\"] = w[1]\n",
    "            weights[labels == \"08\"] = w[2]\n",
    "            if sm == 1:\n",
    "                n = max(\n",
    "                    (labels == \"04\").sum(),\n",
    "                    (labels == \"05\").sum(),\n",
    "                    (labels == \"08\").sum(),\n",
    "                )\n",
    "                sampler = WeightedRandomSampler(weights, int(3 * n))\n",
    "            else:\n",
    "                n = min(\n",
    "                    (labels == \"04\").sum(),\n",
    "                    (labels == \"05\").sum(),\n",
    "                    (labels == \"08\").sum(),\n",
    "                )\n",
    "                print(n, len(labels), type(n))\n",
    "                sampler = WeightedRandomSampler(weights, int(3 * n), replacement=False)\n",
    "        else:\n",
    "            sampler = RandomSampler(self.data.train)\n",
    "        return DataLoader(\n",
    "            self.data.train, batch_size=self.bs, sampler=sampler, drop_last=True\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_nb, optimizer_idx):\n",
    "        x, y = batch\n",
    "        x = _open_functions[self.hparams.open_mode](x).detach()\n",
    "        normalized_imgs = self(x)\n",
    "\n",
    "        mse = self.geometric_loss(normalized_imgs, x)\n",
    "        d_loss = self.loss(self.discriminator(normalized_imgs), y)\n",
    "        g_loss = mse - 0.5 * d_loss\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "            loss = g_loss\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            loss = self.loss(self.discriminator(normalized_imgs.detach()), y)\n",
    "\n",
    "        lr = self.sched.optimizer.param_groups[-1][\"lr\"]\n",
    "        log = {\"d_loss\": d_loss, \"mse\": mse, \"g_loss\": g_loss, \"learning_rate\": lr}\n",
    "        return {\"loss\": loss, \"log\": log}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        x = _open_functions[self.hparams.open_mode](x).detach()\n",
    "        normalized_imgs = self(x)\n",
    "        mse = self.geometric_loss(normalized_imgs, x)\n",
    "        d_loss = self.loss(self.discriminator(normalized_imgs), y)\n",
    "        g_loss = mse - 0.5 * d_loss\n",
    "        return {\n",
    "            \"val_loss\": g_loss,\n",
    "            \"val_d_loss\": d_loss,\n",
    "            \"val_mse\": mse,\n",
    "            \"val_g_loss\": g_loss,\n",
    "        }\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        d_loss = torch.stack([x[\"val_d_loss\"] for x in outputs]).mean()\n",
    "        g_loss = torch.stack([x[\"val_g_loss\"] for x in outputs]).mean()\n",
    "        mse = torch.stack([x[\"val_mse\"] for x in outputs]).mean()\n",
    "        log = {\"val_d_loss\": d_loss, \"val_mse\": mse, \"val_g_loss\": g_loss}\n",
    "        return {\"val_loss\": g_loss, \"log\": log}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.opt = torch.optim.Adam(self.unet.parameters(), lr=self.lr)\n",
    "        self.sched = _get_scheduler(\n",
    "            self.opt,\n",
    "            self.hparams.sched,\n",
    "            self.hparams.epochs * len(self.train_dataloader()),\n",
    "            self.lr,\n",
    "        )\n",
    "\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "        return [self.opt, opt_d], [self.sched]\n",
    "\n",
    "    def show_results(self, n=16, imgsize=4, title=None):\n",
    "        \"\"\"\n",
    "        Plot `n` predictions from the normalizer. Each line will contain input, target and prediction\n",
    "        images (in that order).\n",
    "        \"\"\"\n",
    "        n = min(n, self.bs)\n",
    "        fig, axs = plt.subplots(n, 2, figsize=(imgsize * 2, imgsize * n))\n",
    "        idxs = np.random.choice(np.arange(len(self.data.valid)), size=n, replace=False)\n",
    "        inputs = []\n",
    "        targs = []\n",
    "        for idx in idxs:\n",
    "            x, y = self.data.valid[idx]\n",
    "            inputs.append(x)\n",
    "            targs.append(y)\n",
    "        inputs = torch.stack(inputs).to(next(self.parameters()).device)\n",
    "        normalized = self.predict(inputs).clamp(0, 1).detach()\n",
    "        preds = self.discriminator(normalized).detach().argmax(1)\n",
    "        for ax_r, x, y, p, z in zip(axs, inputs, targs, preds, normalized):\n",
    "            x = x.cpu().numpy().transpose(1, 2, 0)\n",
    "            y = y.item()\n",
    "            p = p.item()\n",
    "            z = z.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "            show_img(x, ax=ax_r[0], title=self.data.train._ds.label_loader.classes[y])\n",
    "            show_img(z, ax=ax_r[1], title=self.data.train._ds.label_loader.classes[p])\n",
    "        title = ifnone(title, \"input/target/prediction\")\n",
    "        fig.suptitle(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Normalizer.show_results\" class=\"doc_header\"><code>Normalizer.show_results</code><a href=\"__main__.py#L27\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Normalizer.show_results</code>(**`n`**=*`16`*, **`imgsize`**=*`4`*, **`title`**=*`None`*)\n",
       "\n",
       "Plot `n` predictions from the normalizer. Each line will contain input, target and prediction\n",
       "images (in that order). "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Normalizer.show_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Normalizer.freeze_encoder\" class=\"doc_header\"><code>Normalizer.freeze_encoder</code><a href=\"__main__.py#L54\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Normalizer.freeze_encoder</code>()\n",
       "\n",
       "Freeze the encoder part of the normalizer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Normalizer.freeze_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Normalizer.init_bn\" class=\"doc_header\"><code>Normalizer.init_bn</code><a href=\"__main__.py#L63\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Normalizer.init_bn</code>()\n",
       "\n",
       "Initialize BatchNorm layers with bias `1e-3` and weights `1`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Normalizer.init_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DiscrimDataModule(BaseDataModule):\n",
    "    def __init__(self, sample_mode: int = 0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sample_mode = sample_mode\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        def label_func(x):\n",
    "            if \"PACS04\" in x.name:\n",
    "                return \"04\"\n",
    "            elif \"PACS05\" in x.name:\n",
    "                return \"05\"\n",
    "            else:\n",
    "                return \"08\"\n",
    "\n",
    "        self.filt = self.get_filt()\n",
    "        data = ImageClassifDataset.from_folder(\n",
    "            self.datafolder,\n",
    "            label_func,\n",
    "            classes=[\"04\", \"05\", \"08\"],\n",
    "            extensions=[\".png\"],\n",
    "            include=[\"1\", \"3\"],\n",
    "            filterfunc=self.filt,\n",
    "        ).split_by_csv(self.data_csv)\n",
    "        n = len(data.train)\n",
    "        weights = [\n",
    "            np.float32(n / (data.train.labels == \"04\").sum()),\n",
    "            np.float32(n / (data.train.labels == \"05\").sum()),\n",
    "            np.float32(n / (data.train.labels == \"08\").sum()),\n",
    "        ]\n",
    "        self.weights = weights\n",
    "        self.data = data.to_tensor(tfms=self.tfms, tfm_y=False)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        sm = self.sample_mode\n",
    "        w = self.weights\n",
    "        labels = self.data.train.labels\n",
    "        if sm > 0:\n",
    "            weights = np.where(labels == \"04\", w[0], 1.0)\n",
    "            weights[labels == \"05\"] = w[1]\n",
    "            weights[labels == \"08\"] = w[2]\n",
    "            if sm == 1:\n",
    "                n = max(\n",
    "                    (labels == \"04\").sum(),\n",
    "                    (labels == \"05\").sum(),\n",
    "                    (labels == \"08\").sum(),\n",
    "                )\n",
    "                sampler = WeightedRandomSampler(weights, int(3 * n))\n",
    "            else:\n",
    "                n = min(\n",
    "                    (labels == \"04\").sum(),\n",
    "                    (labels == \"05\").sum(),\n",
    "                    (labels == \"08\").sum(),\n",
    "                )\n",
    "                print(n, len(labels), type(n))\n",
    "                sampler = WeightedRandomSampler(weights, int(3 * n), replacement=False)\n",
    "        else:\n",
    "            sampler = RandomSampler(self.data.train)\n",
    "        return DataLoader(\n",
    "            self.data.train,\n",
    "            batch_size=self.bs,\n",
    "            sampler=sampler,\n",
    "            drop_last=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PACSDiscriminator(BaseModule):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        hparams = self.hparams\n",
    "        self.loss = _get_loss(\n",
    "            hparams.loss, 1.0, hparams.reduction, device=self.main_device, nc=3\n",
    "        )\n",
    "        if \"cbr\" in hparams.model:\n",
    "            args = map(int, hparams.model.split(\"_\")[1:])\n",
    "            base_model = CBR(*args)\n",
    "            cut = -3\n",
    "        elif \"sasa\" in hparams.model:\n",
    "            args = map(int, hparams.model.split(\"_\")[1:])\n",
    "            base_model = SASA(*args)\n",
    "            cut = -3\n",
    "        elif \"sanet\" in hparams.model:\n",
    "            splits = hparams.model.split(\"_\")\n",
    "            kernel_size = int(splits[-1])\n",
    "            base_model = globals()[splits[0]](kernel_size)\n",
    "            cut = -2\n",
    "        else:\n",
    "            base_model = timm.create_model(\n",
    "                hparams.model, pretrained=not hparams.rand_weights\n",
    "            )\n",
    "            cut = -2\n",
    "        self.base_model = nn.Sequential(*list(base_model.children())[:cut])\n",
    "        head = [nn.AdaptiveAvgPool2d(1), nn.Flatten()]\n",
    "        nf = get_num_features(self.base_model)\n",
    "        p = hparams.dropout\n",
    "        head += bn_drop_lin(nf, nf, p=p / 2) + bn_drop_lin(nf, 3, p=p)\n",
    "        self.head = nn.Sequential(*head)\n",
    "        self.post_init()\n",
    "        self._create_normalizer()\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        ret = {\"loss\": loss}\n",
    "        n = y.shape[0]\n",
    "        y_hat = torch.softmax(y_hat, dim=1)\n",
    "        y_hat = y_hat.argmax(dim=-1).view(n, -1)\n",
    "        y = y.view(n, -1)\n",
    "        ret[\"t0\"] = ((y_hat == 0) & (y == 0)).float().sum()\n",
    "        ret[\"t1\"] = ((y_hat == 1) & (y == 1)).float().sum()\n",
    "        ret[\"t2\"] = ((y_hat == 2) & (y == 2)).float().sum()\n",
    "        ret[\"f0\"] = ((y_hat == 0) & (y != 0)).float().sum()\n",
    "        ret[\"f1\"] = ((y_hat == 1) & (y != 1)).float().sum()\n",
    "        ret[\"f2\"] = ((y_hat == 2) & (y != 2)).float().sum()\n",
    "        return ret\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Dict[str, torch.Tensor]]):\n",
    "        # OPTIONAL\n",
    "        log = {}\n",
    "        t0, f0, t1, f1, t2, f2 = _get_keys_in_list_and_apply(\n",
    "            outputs, \"t0\", \"f0\", \"t1\", \"f1\", \"t2\", \"f2\", apply_func=torch.sum\n",
    "        )\n",
    "        t = torch.stack((t0, t1, t2))\n",
    "        f = torch.stack((f0, f1, f2))\n",
    "        for metric in self.metrics:\n",
    "            try:\n",
    "                name = metric.__name__\n",
    "            except AttributeError:\n",
    "                name = metric.func.__name__\n",
    "                kws = metric.keywords\n",
    "                for k in kws:\n",
    "                    name += f\"_{k}_{kws[k]}\"\n",
    "            for c in range(3):\n",
    "                c_name = name + f\"_{c}\"\n",
    "                log[c_name] = metric(\n",
    "                    t[c],\n",
    "                    f[c],\n",
    "                    t[:c].sum() + t[c + 1 :].sum(),\n",
    "                    f[:c].sum() + f[c + 1 :].sum(),\n",
    "                )\n",
    "        log[\"val_loss\"] = torch.mean([output[\"loss\"] for output in outputs])\n",
    "        outputs.log_dict(log)\n",
    "\n",
    "    def _create_normalizer(self):\n",
    "        hparams = self.hparams\n",
    "        if hparams.normalizer is not None:\n",
    "            norm = DynamicUnet(\n",
    "                hparams.normalizer,\n",
    "                n_classes=3,\n",
    "                input_shape=(3, hparams.size, hparams.size),\n",
    "                pretrained=True,\n",
    "            )\n",
    "            if hparams.norm_version is not None:\n",
    "                save_dir = (\n",
    "                    self.save_path.parents[1]\n",
    "                    / \"normalizer\"\n",
    "                    / f\"{hparams.normalizer}/lightning_logs\"\n",
    "                    / \"version_{hparams.norm_version}/checkpoints\"\n",
    "                )\n",
    "                path = next(save_dir.iterdir())\n",
    "                ckpt = torch.load(path, map_location=lambda stor, loc: stor)\n",
    "                state_dict = {}\n",
    "                for k in ckpt[\"state_dict\"]:\n",
    "                    state_dict[k.replace(\"unet.\", \"\")] = ckpt[\"state_dict\"][k]\n",
    "                norm.load_state_dict(state_dict)\n",
    "                for p in norm.parameters():\n",
    "                    p.requires_grad = False\n",
    "            norm = norm.to(self.main_device)\n",
    "            self.norm = norm.__call__\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = _open_functions[self.hparams.open_mode](x).detach()\n",
    "        if hasattr(self, \"norm\"):\n",
    "            x = self.norm(x)\n",
    "        x = self.base_model(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pred = super().predict(x)\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        return pred\n",
    "\n",
    "    def freeze_base(self):\n",
    "        \"\"\"\n",
    "        Freeze the base model.\n",
    "        \"\"\"\n",
    "        for m in self.leaf_modules:\n",
    "            if \"base_model\" in m.name and not isinstance(m, nn.BatchNorm2d):\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GradeClassifDataModule(BaseDataModule):\n",
    "    def __init__(self, sample_mode: int = 0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sample_mode = sample_mode\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.filt = self.get_filt()\n",
    "        self.data = (\n",
    "            ImageClassifDataset.from_folder(\n",
    "                self.datafolder,\n",
    "                lambda x: x.parts[-3],\n",
    "                classes=[\"1\", \"3\"],\n",
    "                extensions=[\".png\"],\n",
    "                include=[\"1\", \"3\"],\n",
    "                filterfunc=self.filt,\n",
    "            )\n",
    "            .split_by_csv(self.data_csv)\n",
    "            .to_tensor(tfms=self.tfms, tfm_y=False)\n",
    "        )\n",
    "        weights = np.float32(\n",
    "            (self.data.train.labels == \"3\").sum()\n",
    "            / (self.data.train.labels == \"1\").sum()\n",
    "        )\n",
    "        self.weights = weights\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        sm = self.sample_mode\n",
    "        w = self.weights\n",
    "        if sm > 0:\n",
    "            labels = self.data.train.labels == \"1\"\n",
    "            weights = np.where(labels, w, 1.0)\n",
    "            if sm == 1:\n",
    "                sampler = WeightedRandomSampler(weights, 2 * len(np.argwhere(~labels)))\n",
    "            else:\n",
    "                # sampler = WeightedRandomSampler(weights,\n",
    "                #                                 2*len(np.argwhere(labels)),\n",
    "                #                                 replacement=False)\n",
    "                sampler = WeightedRandomSampler(\n",
    "                    weights, 40000, replacement=False, num_workers=4\n",
    "                )\n",
    "        else:\n",
    "            sampler = RandomSampler(self.data.train)\n",
    "        return DataLoader(\n",
    "            self.data.train,\n",
    "            batch_size=self.bs,\n",
    "            sampler=sampler,\n",
    "            drop_last=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GradeClassifModel(BaseModule):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        hparams = self.hparams\n",
    "        self.loss = _get_loss(\n",
    "            hparams.loss, 1.0, hparams.reduction, device=self.main_device, nc=2\n",
    "        )\n",
    "        if \"cbr\" in hparams.model:\n",
    "            args = map(int, hparams.model.split(\"_\")[1:])\n",
    "            base_model = CBR(*args)\n",
    "            cut = -3\n",
    "        elif \"sasa\" in hparams.model:\n",
    "            args = map(int, hparams.model.split(\"_\")[1:])\n",
    "            base_model = SASA(*args)\n",
    "            cut = -3\n",
    "        elif \"sanet\" in hparams.model:\n",
    "            splits = hparams.model.split(\"_\")\n",
    "            kernel_size = int(splits[-1])\n",
    "            base_model = globals()[splits[0]](kernel_size)\n",
    "            cut = -2\n",
    "        else:\n",
    "            base_model = timm.create_model(\n",
    "                hparams.model, pretrained=not hparams.rand_weights\n",
    "            )\n",
    "            cut = -2\n",
    "        self.base_model = nn.Sequential(*list(base_model.children())[:cut])\n",
    "        head = [nn.AdaptiveAvgPool2d(1), nn.Flatten()]\n",
    "        nf = get_num_features(self.base_model)\n",
    "        p = hparams.dropout\n",
    "        nc = 2 if hparams.loss == \"cross-entropy\" else 1\n",
    "        head += bn_drop_lin(nf, nf, p=p / 2) + bn_drop_lin(nf, nc, p=p)\n",
    "        self.head = nn.Sequential(*head)\n",
    "        self.post_init()\n",
    "        self._create_normalizer()\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        ret = {\"loss\": loss}\n",
    "        n = y.shape[0]\n",
    "        if self.hparams.loss == \"cross-entropy\":\n",
    "            y_hat = torch.softmax(y_hat, dim=1)\n",
    "            y_hat = y_hat.argmax(dim=-1).view(n, -1)\n",
    "        else:\n",
    "            y_hat = torch.sigmoid(y_hat)\n",
    "            y_hat = (y_hat > 0.5).view(n, -1)\n",
    "        y = y.view(n, -1)\n",
    "        ret[\"tp\"] = ((y_hat) & (y == 1)).float().sum()\n",
    "        ret[\"tn\"] = ((~y_hat) & (y == 0)).float().sum()\n",
    "        ret[\"fp\"] = ((y_hat) & (y == 0)).float().sum()\n",
    "        ret[\"fn\"] = ((~y_hat) & (y == 1)).float().sum()\n",
    "        return ret\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Dict[str, torch.Tensor]]):\n",
    "        # OPTIONAL\n",
    "        tp, fp, tn, fn = _get_keys_in_list_and_apply(\n",
    "            outputs, \"tp\", \"fp\", \"tn\", \"fn\", apply_func=torch.sum\n",
    "        )\n",
    "        log = {}\n",
    "        for metric in self.metrics:\n",
    "            try:\n",
    "                name = metric.__name__\n",
    "            except AttributeError:\n",
    "                name = metric.func.__name__\n",
    "                kws = metric.keywords\n",
    "                for k in kws:\n",
    "                    name += f\"_{k}_{kws[k]}\"\n",
    "            log[name] = metric(tp, fp, tn, fn)\n",
    "        log[\"val_loss\"] = torch.mean([output[\"loss\"] for output in outputs])\n",
    "        self.log_dict(log)\n",
    "\n",
    "    def _create_normalizer(self):\n",
    "        hparams = self.hparams\n",
    "        if hparams.normalizer is not None:\n",
    "            norm = DynamicUnet(\n",
    "                hparams.normalizer,\n",
    "                n_classes=3,\n",
    "                input_shape=(3, hparams.size, hparams.size),\n",
    "                pretrained=True,\n",
    "            )\n",
    "            if hparams.norm_version is not None:\n",
    "                save_dir = (\n",
    "                    self.save_path.parents[1]\n",
    "                    / \"normalizer\"\n",
    "                    / f\"{hparams.normalizer}/lightning_logs\"\n",
    "                    / \"version_{hparams.norm_version}/checkpoints\"\n",
    "                )\n",
    "                path = next(save_dir.iterdir())\n",
    "                ckpt = torch.load(path, map_location=lambda stor, loc: stor)\n",
    "                state_dict = {}\n",
    "                for k in ckpt[\"state_dict\"]:\n",
    "                    state_dict[k.replace(\"unet.\", \"\")] = ckpt[\"state_dict\"][k]\n",
    "                norm.load_state_dict(state_dict)\n",
    "                for p in norm.parameters():\n",
    "                    p.requires_grad = False\n",
    "            norm = norm.to(self.main_device)\n",
    "            self.norm = norm.__call__\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = _open_functions[self.hparams.open_mode](x).detach()\n",
    "        if hasattr(self, \"norm\"):\n",
    "            x = self.norm(x)\n",
    "        x = self.base_model(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pred = super().predict(x)\n",
    "        if self.hparams.loss == \"cross-entropy\":\n",
    "            pred = torch.softmax(pred, dim=1)\n",
    "        else:\n",
    "            pred = torch.sigmoid(pred)\n",
    "        return pred\n",
    "\n",
    "    def freeze_base(self):\n",
    "        \"\"\"\n",
    "        Freeze the base model.\n",
    "        \"\"\"\n",
    "        for m in self.leaf_modules:\n",
    "            if \"base_model\" in m.name and not isinstance(m, nn.BatchNorm2d):\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module containing a model used to make cancer grade classification. Model can either be a usual model defined in [`timm`](https://github.com/rwightman/pytorch-image-models), a `CBR` (for Conv-BatchNorm-ReLU), a `SASA` (for Stand Alone Self-Attention) or a `SANet` (that mixes SASA and ResNet). All models heads are replaced by a GlobalAveragePool followed by two sequences of BatchNorm-Dropout-Linear layers. Dropout rate defined in `hparams` is halfed for the first sequence.\n",
    "\n",
    "All important parameters are contained in `hparams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RNNAttention(BaseModule):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_glimpses: int = 3, glimpse_size: int = 256, gamma: float = 1, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        hparams = self.hparams\n",
    "        self.loss = _get_loss(\n",
    "            hparams.loss, 1.0, hparams.reduction, device=self.main_device\n",
    "        )\n",
    "        nc = 2 if hparams.loss == \"cross-entropy\" else 1\n",
    "        self.t_x = nn.Sequential(*list(CBR(3, 64, 2).children())[:-1])\n",
    "        nf = get_num_features(self.t_x)\n",
    "        self.fc = nn.Linear(nx, nc)\n",
    "        self.t_l = nn.Sequential(\n",
    "            nn.Linear(6, nf),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nf, 2 * nf),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * nf, nf),\n",
    "        )\n",
    "        self.t_a = nn.Sequential(\n",
    "            nn.Linear(nf, 2 * nf),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * nf, nf),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nf, 6),\n",
    "        )\n",
    "        self.final_head = nn.Sequential(\n",
    "            nn.Linear(nx * n_glimpses, nx), nn.ReLU(), nn.Linear(nf, nc)\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, l: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        X = _open_functions[self.hparams.open_mode](X).detach()\n",
    "        Ai = gaussian_mask(*l[:3], self.hparams.glimpse_size, self.hparams.size)\n",
    "        Aj = gaussian_mask(*l[3:], self.hparams.glimpse_size, self.hparams.size)\n",
    "        x = Ai[None, None] @ X @ Aj.T[None, None]\n",
    "        if hasattr(self, \"norm\"):\n",
    "            x = self.norm(x)\n",
    "        fx = self.t_x(x)\n",
    "        y = self.fc(fx)\n",
    "        fl = self.t_l(l)\n",
    "        l = torch.sigmoid(fx * fl)\n",
    "        l = self.t_a(l)\n",
    "        return fx, y, l\n",
    "\n",
    "    def compute_loss(\n",
    "        self, X: torch.Tensor, Y: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the loss defined in the paper. Return the final slide-level\n",
    "        prediction as well.\n",
    "        \"\"\"\n",
    "        l0 = torch.tensor(\n",
    "            [0, 0.5, self.hparams.size // self.hparams.glimpse_size] * 2,\n",
    "            device=self.main_device,\n",
    "        )\n",
    "        loss = 0\n",
    "        loss_prev = 0\n",
    "        preds = []\n",
    "        for t in range(self.hparams.n_glimpses):\n",
    "            fx, y_hat, l = self(X, l0)\n",
    "            fts.append(fx)\n",
    "            loss += self.loss(y_hat, Y)\n",
    "            loss_a = (y_hat ** 2).sum()\n",
    "            if t > 0:\n",
    "                loss -= loss_a - loss_prev / t\n",
    "            loss_prev += loss_a\n",
    "            loss += self.hparams.gamma * torch.exp(-torch.abs(l - l0))\n",
    "            l0 = l.detach()\n",
    "        # n_glimpses x bs x C\n",
    "        fts = torch.cat(fts)\n",
    "        fts = fts.view(fts.shape[1], -1)\n",
    "        Y_hat = self.final_head(fts)\n",
    "        loss += self.loss(Y_hat, Y)\n",
    "        return loss, Y_hat\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # REQUIRED\n",
    "        X, Y = batch\n",
    "        loss, _ = self.compute_loss(X, Y)\n",
    "        lr = self.sched.optimizer.param_groups[-1][\"lr\"]\n",
    "        log = {\"train_loss\": loss, \"learning_rate\": lr}\n",
    "        self.log_dict(log)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        ret = {\"loss\": loss}\n",
    "        n = y.shape[0]\n",
    "        if self.hparams.loss == \"cross-entropy\":\n",
    "            y_hat = torch.softmax(y_hat, dim=1)\n",
    "            y_hat = y_hat.argmax(dim=-1).view(n, -1)\n",
    "        else:\n",
    "            y_hat = torch.sigmoid(y_hat)\n",
    "            y_hat = (y_hat > 0.5).view(n, -1)\n",
    "        y = y.view(n, -1)\n",
    "        ret[\"tp\"] = ((y_hat) & (y == 1)).float().sum()\n",
    "        ret[\"tn\"] = ((~y_hat) & (y == 0)).float().sum()\n",
    "        ret[\"fp\"] = ((y_hat) & (y == 0)).float().sum()\n",
    "        ret[\"fn\"] = ((~y_hat) & (y == 1)).float().sum()\n",
    "        return ret\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Dict[str, torch.Tensor]]):\n",
    "        # OPTIONAL\n",
    "        tp, fp, tn, fn = _get_keys_in_list_and_apply(\n",
    "            outputs, \"tp\", \"fp\", \"tn\", \"fn\", apply_func=torch.sum\n",
    "        )\n",
    "        log = {}\n",
    "        for metric in self.metrics:\n",
    "            try:\n",
    "                name = metric.__name__\n",
    "            except AttributeError:\n",
    "                name = metric.func.__name__\n",
    "                kws = metric.keywords\n",
    "                for k in kws:\n",
    "                    name += f\"_{k}_{kws[k]}\"\n",
    "            log[name] = metric(tp, fp, tn, fn)\n",
    "        log[\"val_loss\"] = torch.mean([output[\"loss\"] for output in outputs])\n",
    "        self.log_dict(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN model with attention as defined in [_Predicting Cancer with a Recurrent Visual Attention Model for Histopathology Images_](http://www.sfu.ca/~abentaie/papers/miccai18.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"RNNAttention.compute_loss\" class=\"doc_header\"><code>RNNAttention.compute_loss</code><a href=\"__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>RNNAttention.compute_loss</code>(**`X`**, **`Y`**)\n",
       "\n",
       "Compute the loss defined in the paper. Return the final slide-level prediction as well."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RNNAttention.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data.read.ipynb.\n",
      "Converted 11_data.loaders.ipynb.\n",
      "Converted 12_data.dataset.ipynb.\n",
      "Converted 13_data.utils.ipynb.\n",
      "Converted 14_data.transforms.ipynb.\n",
      "Converted 15_data.color.ipynb.\n",
      "Converted 20_models.plmodules.ipynb.\n",
      "Converted 21_models.modules.ipynb.\n",
      "Converted 22_models.utils.ipynb.\n",
      "Converted 23_models.hooks.ipynb.\n",
      "Converted 24_models.metrics.ipynb.\n",
      "Converted 25_models.losses.ipynb.\n",
      "Converted 80_params.defaults.ipynb.\n",
      "Converted 81_params.parser.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
