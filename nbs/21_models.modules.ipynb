{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from grade_classif.models.utils import get_sizes\n",
    "from grade_classif.models.hooks import Hooks\n",
    "from grade_classif.imports import *\n",
    "from torch.nn.functional import interpolate, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvBnRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, bias=True, eps=1e-5, momentum=0.01, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride=stride,\n",
    "            padding=padding, bias=bias, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_channels, eps=eps, momentum=momentum)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, bias=True, eps=1e-5, momentum=0.01, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride=stride,\n",
    "            padding=padding, bias=bias, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_channels, eps=eps, momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "            bias=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride=stride,\n",
    "            padding=padding, bias=bias, **kwargs)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def icnr(x, scale=2, init=nn.init.kaiming_normal_):\n",
    "    ni, nf, h, w = x.shape\n",
    "    ni2 = int(ni/(scale**2))\n",
    "    k = init(torch.zeros([ni2, nf, h, w])).transpose(0, 1)\n",
    "    k = k.contiguous().view(ni2, nf, -1)\n",
    "    k = k.repeat(1, 1, scale**2)\n",
    "    k = k.contiguous().view([nf, ni, h, w]).transpose(0, 1)\n",
    "    x.data.copy_(k)\n",
    "\n",
    "class PixelShuffleICNR(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, bias=True, scale_factor=2, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels*scale_factor**2, 1, bias=bias, **kwargs)\n",
    "        icnr(self.conv.weight)\n",
    "        self.shuf = nn.PixelShuffle(scale_factor)\n",
    "        # self.pad = nn.ReflectionPad2d((1, 0, 1, 0))\n",
    "        # self.blur = nn.AvgPool2d(2, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.shuf(x)\n",
    "        # x = self.pad(x)\n",
    "        # x = self.blur(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_chans, skip_chans, hook, final_div=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.shuf = PixelShuffleICNR(in_chans, in_chans//2, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(skip_chans)\n",
    "        ni = in_chans//2 + skip_chans\n",
    "        nf = ni if not final_div else skip_chans\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = ConvBnRelu(ni, nf, 3, padding=1, **kwargs)\n",
    "        self.conv2 = ConvBnRelu(nf, nf, 3, padding=1, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skipco = self.hook.stored\n",
    "        x = self.shuf(x)\n",
    "        ssh = skipco.shape[-2:]\n",
    "        if ssh != x.shape[-2:]:\n",
    "            x = interpolate(x, ssh, mode='nearest')\n",
    "        x = self.relu(torch.cat([x, self.bn(skipco)], dim=1))\n",
    "        return self.conv2(self.conv1(x))\n",
    "    \n",
    "class LastCross(nn.Module):\n",
    "    def __init__(self, n_chans, bottle=False):\n",
    "        super(LastCross, self).__init__()\n",
    "        n_mid = n_chans//2 if bottle else n_chans\n",
    "        self.conv1 = ConvBnRelu(n_chans, n_mid, 3, padding=1)\n",
    "        self.conv2 = ConvBnRelu(n_mid, n_chans, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DynamicUnet(nn.Module):\n",
    "    def __init__(self, encoder_name, cut=-2, n_classes=2, input_shape=(3, 224, 224), pretrained=True):\n",
    "        super().__init__()\n",
    "        encoder = timm.create_model(encoder_name, pretrained=pretrained)\n",
    "        # encoder = resnet34()\n",
    "        self.encoder = nn.Sequential(*(list(encoder.children())[:cut]+[nn.ReLU()]))\n",
    "        encoder_sizes, idxs = self.register_output_hooks(input_shape=input_shape)\n",
    "        n_chans = encoder_sizes[-1][1]\n",
    "        middle_conv = nn.Sequential(ConvBnRelu(n_chans, n_chans//2, 3),\n",
    "                                    ConvBnRelu(n_chans//2, n_chans, 3))\n",
    "        decoder = [middle_conv]\n",
    "        for k, (idx, hook) in enumerate(zip(idxs[::-1], self.hooks)):\n",
    "            skip_chans = encoder_sizes[idx][1]\n",
    "            final_div = (k != len(idxs)-1)\n",
    "            decoder.append(DecoderBlock(n_chans, skip_chans, hook, final_div=final_div))\n",
    "            n_chans = n_chans//2 + skip_chans\n",
    "            n_chans = n_chans if not final_div else skip_chans\n",
    "        self.decoder = nn.Sequential(*decoder, PixelShuffleICNR(n_chans, n_chans))\n",
    "        n_chans += input_shape[0]\n",
    "        self.head = nn.Sequential(LastCross(n_chans), nn.Conv2d(n_chans, n_classes, 1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.encoder(x)\n",
    "        y = self.decoder(y)\n",
    "        if y.shape[-2:] != x.shape[-2:]:\n",
    "            y = interpolate(y, x.shape[-2:], mode='nearest')\n",
    "        y = torch.cat([x, y], dim=1)\n",
    "        y = self.head(y)\n",
    "        return y\n",
    "    \n",
    "        \n",
    "    def register_output_hooks(self, input_shape=(3, 224, 224)):\n",
    "        sizes, modules = get_sizes(self.encoder, input_shape=input_shape)\n",
    "        mods = []\n",
    "        idxs = np.where(sizes[:-1, -1] != sizes[1:, -1])[0]\n",
    "        def _hook(model, input, output):\n",
    "            return output\n",
    "                \n",
    "        for k in idxs[::-1]:\n",
    "            out_shape = sizes[k]\n",
    "            m = modules[k]\n",
    "            if 'downsample' not in m.name:\n",
    "                mods.append(m)\n",
    "        self.hooks = Hooks(mods, _hook)\n",
    "        \n",
    "        return sizes, idxs\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"hooks\"): self.hooks.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CBR(nn.Module):\n",
    "    def __init__(self, kernel_size, n_kernels, n_layers, n_classes=2):\n",
    "        super().__init__()\n",
    "        in_c = 3\n",
    "        out_c = n_kernels\n",
    "        for k in range(n_layers):\n",
    "            self.add_module(f'cbr{k}', ConvBnRelu(in_c, out_c, kernel_size, stride=2, padding=kernel_size//2, padding_mode='reflect'))\n",
    "            # self.add_module(f'maxpool{k}', nn.MaxPool2d(3, stride=2, padding=1))\n",
    "            in_c = out_c\n",
    "            out_c *= 2\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flat = nn.Flatten(-2, -1)\n",
    "        self.fc = nn.Linear(out_c, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for m in self.children():\n",
    "            x = m(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBR(\n",
       "  (cbr0): ConvBnRelu(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), padding_mode=reflect)\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (cbr1): ConvBnRelu(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), padding_mode=reflect)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (cbr2): ConvBnRelu(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), padding_mode=reflect)\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (cbr3): ConvBnRelu(\n",
       "    (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), padding_mode=reflect)\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
       "  (flat): Flatten()\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = CBR(7, 32, 4); mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k, stride=1, groups=1, bias=False):\n",
    "        super().__init__()\n",
    "        assert c_in % groups == c_out % groups == 0, \"c_in and c_out must be divided by groups\"\n",
    "        assert k % 2 == 1, \"k must be odd\"\n",
    "        assert c_out % 2 == 0, \"c_out must be even\"\n",
    "        \n",
    "        padding = k // 2\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        \n",
    "        self.key_conv = nn.Conv2d(c_in, c_out, 1, padding=padding, groups=groups, bias=bias, padding_mode='reflect')\n",
    "        self.query_conv = nn.Conv2d(c_in, c_out, 1, groups=groups, bias=bias)\n",
    "        self.value_conv = nn.Conv2d(c_in, c_out, 1, padding=padding, groups=groups, bias=bias, padding_mode='reflect')\n",
    "        \n",
    "        self.r_ai = nn.Parameter(torch.randn(1, c_out//2, k, 1))\n",
    "        self.r_aj = nn.Parameter(torch.randn(1, c_out//2, 1, k))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        n = self.c_out // self.groups\n",
    "        \n",
    "        q = self.query_conv(x).view(b, self.groups, n, h, w, 1)\n",
    "        k = self.key_conv(x).unfold(2, self.k, self.stride).unfold(3, self.k, self.stride).view(b, self.groups, n, h, w, -1)\n",
    "        v = self.value_conv(x).unfold(2, self.k, self.stride).unfold(3, self.k, self.stride).view(b, self.groups, n, h, w, -1)\n",
    "        \n",
    "        r = torch.cat((self.r_ai.expand(b, -1, -1, self.k), self.r_aj.expand(b, -1, self.k, -1)), dim=1).view(b, self.groups, n, -1)\n",
    "        r = r[..., None, None, :].expand(-1, -1, -1, h, w, -1)\n",
    "        \n",
    "        y = (torch.softmax((q*(k+r)).sum(2, keepdims=True), dim=-1) * v).sum(-1).view(b, c_out, h, w)\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data.read.ipynb.\n",
      "Converted 11_data.loaders.ipynb.\n",
      "Converted 12_data.dataset.ipynb.\n",
      "Converted 13_data.utils.ipynb.\n",
      "Converted 14_data.transforms.ipynb.\n",
      "Converted 20_models.plmodules.ipynb.\n",
      "Converted 21_models.modules.ipynb.\n",
      "Converted 22_models.utils.ipynb.\n",
      "Converted 23_models.hooks.ipynb.\n",
      "Converted 24_models.metrics.ipynb.\n",
      "Converted 25_models.losses.ipynb.\n",
      "Converted 80_params.defaults.ipynb.\n",
      "Converted 81_params.parser.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
