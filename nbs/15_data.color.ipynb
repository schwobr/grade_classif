{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "> Utilities for image transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from grade_classif.imports import *\n",
    "from scipy import linalg\n",
    "from kornia.color import rgb_to_xyz, rgb_to_grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_xyz_ref_white = (.950456, 1., 1.088754)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rgb_to_lab(image: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    r\"\"\"Converts a RGB image to Luv.\n",
    "\n",
    "    See :class:`~kornia.color.RgbToLuv` for details.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): RGB image\n",
    "        eps (float): for numerically stability when dividing. Default: 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor : Luv image\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.is_tensor(image):\n",
    "        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(type(image)))\n",
    "\n",
    "    if len(image.shape) < 3 or image.shape[-3] != 3:\n",
    "        raise ValueError(\n",
    "            \"Input size must have a shape of (*, 3, H, W). Got {}\".format(image.shape)\n",
    "        )\n",
    "\n",
    "    # Convert from Linear RGB to sRGB\n",
    "    r = image[..., 0, :, :]\n",
    "    g = image[..., 1, :, :]\n",
    "    b = image[..., 2, :, :]\n",
    "\n",
    "    rs = torch.where(r > 0.04045, torch.pow(((r + 0.055) / 1.055), 2.4), r / 12.92)\n",
    "    gs = torch.where(g > 0.04045, torch.pow(((g + 0.055) / 1.055), 2.4), g / 12.92)\n",
    "    bs = torch.where(b > 0.04045, torch.pow(((b + 0.055) / 1.055), 2.4), b / 12.92)\n",
    "\n",
    "    image_s = torch.stack((rs, gs, bs), dim=-3)\n",
    "\n",
    "    xyz_im: torch.Tensor = rgb_to_xyz(image)\n",
    "\n",
    "    x = xyz_im[..., 0, :, :]\n",
    "    y = xyz_im[..., 1, :, :]\n",
    "    z = xyz_im[..., 2, :, :]\n",
    "\n",
    "    x = x / _xyz_ref_white[0]\n",
    "    z = z / _xyz_ref_white[2]\n",
    "\n",
    "    L = torch.where(\n",
    "        torch.gt(y, 0.008856), 116.0 * torch.pow(y, 1.0 / 3.0) - 16.0, 903.3 * y\n",
    "    )\n",
    "\n",
    "    def _f(t):\n",
    "        return torch.where(\n",
    "            torch.gt(t, 0.008856), torch.pow(t, 1 / 3), 7.787 * t + 4 / 29\n",
    "        )\n",
    "\n",
    "    # Compute reference white point\n",
    "    a = 500 * (_f(x) - _f(y))\n",
    "    b = 200 * (_f(y) - _f(z))\n",
    "\n",
    "    out = torch.stack((L, a, b), dim=-3)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_rgb_from_hed = torch.tensor([[0.65, 0.70, 0.29],\n",
    "                              [0.07, 0.99, 0.11],\n",
    "                              [0.27, 0.57, 0.78]], dtype=torch.float32)\n",
    "_hed_from_rgb = torch.inverse(_rgb_from_hed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rgb_to_hed(image: torch.Tensor) -> torch.Tensor:\n",
    "    if len(image.shape) == 4:\n",
    "        perm1 = (0, 2, 3, 1)\n",
    "        perm2 = (0, 3, 1, 2)\n",
    "    else:\n",
    "        perm1 = (1, 2, 0)\n",
    "        perm2 = (2, 0, 1)\n",
    "    image += 2\n",
    "    stains = -(torch.log(image)/np.log(10)).permute(*perm1) @ _hed_from_rgb.to(\n",
    "        device=image.device, dtype=image.dtype\n",
    "    )\n",
    "    return stains.permute(*perm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rgb_to_h(image: torch.Tensor) -> torch.Tensor:\n",
    "    if len(image.shape) == 4:\n",
    "        h = rgb_to_hed(image)[:, 0]\n",
    "        h = (h + 0.7) / 0.46\n",
    "        return torch.stack((h, h, h), axis=1)\n",
    "    else:\n",
    "        h = rgb_to_hed(image)[0]\n",
    "        h = (h + 0.7) / 0.46\n",
    "        return torch.stack((h, h, h), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rgb_to_e(image: torch.Tensor) -> torch.Tensor:\n",
    "    if len(image.shape) == 4:\n",
    "        e = rgb_to_hed(image)[:, 1]\n",
    "        e = (e + 0.1) / 0.47\n",
    "        return torch.stack((e, e, e), axis=1)\n",
    "    else:\n",
    "        e = rgb_to_hed(image)[1]\n",
    "        e = (e + 0.1) / 0.47\n",
    "        return torch.stack((e, e, e), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rgb_to_heg(image: torch.Tensor) -> torch.Tensor:\n",
    "    gray = rgb_to_grayscale(image)\n",
    "    hed = rgb_to_hed(image)\n",
    "    if len(image.shape) == 4:\n",
    "        h, e = hed[:, 0], hed[:, 1]\n",
    "        h = (h + 0.7) / 0.46\n",
    "        e = (e + 0.1) / 0.47\n",
    "        return torch.stack((h, e, gray.squeeze(1)), axis=1)\n",
    "    else:\n",
    "        h, e = hed[0], hed[1]\n",
    "        h = (h + 0.7) / 0.46\n",
    "        e = (e + 0.1) / 0.47\n",
    "        return torch.stack((h, e, gray.squeeze(0)), axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data.read.ipynb.\n",
      "Converted 11_data.loaders.ipynb.\n",
      "Converted 12_data.dataset.ipynb.\n",
      "Converted 13_data.utils.ipynb.\n",
      "Converted 14_data.transforms.ipynb.\n",
      "Converted 15_data.color.ipynb.\n",
      "Converted 20_models.plmodules.ipynb.\n",
      "Converted 21_models.modules.ipynb.\n",
      "Converted 22_models.utils.ipynb.\n",
      "Converted 23_models.hooks.ipynb.\n",
      "Converted 24_models.metrics.ipynb.\n",
      "Converted 25_models.losses.ipynb.\n",
      "Converted 80_params.defaults.ipynb.\n",
      "Converted 81_params.parser.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
