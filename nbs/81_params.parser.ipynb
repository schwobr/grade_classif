{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp params.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from grade_classif.params.defaults import *\n",
    "from grade_classif.imports import *\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--oversample'], dest='oversample', nargs=0, const=True, default=False, type=None, choices=None, help='specify to enable oversampling instead of weighting loss', metavar=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "_parser = ArgumentParser()\n",
    "_parser.add_argument('--file', '-f', help='notebook convenience')\n",
    "_parser.add_argument('--HistoryManager.hist_file', help='nbconvert convenience')\n",
    "_parser.add_argument('--sched', default=SCHED, choices=['one-cycle', 'cosine-anneal', 'reduce-on-plateau', 'none'], help='scheduler for the optimizer')\n",
    "_parser.add_argument('--data', default=DATA, help='path to folder containing data')\n",
    "_parser.add_argument('--data-csv', default=DATA_CSV, help='path to csv listing scans with their grades and split')\n",
    "_parser.add_argument('--concepts', default=CONCEPTS, help='path to csv listing pathes with their corresponding concept')\n",
    "_parser.add_argument('--concept-classes', default=CONCEPT_CLASSES, help='path to csv giving a class for each concept')\n",
    "_parser.add_argument('--patch-classes', default=PATCH_CLASSES, help='path to csv giving a class for each patcgh; overrides concepts and concept-classes if specified.')\n",
    "_parser.add_argument('--level', default=LEVEL, type=int, help='zoom level to work on')\n",
    "_parser.add_argument('--full-data', default=FULL_DATA, help='path to folder containing all data (different levels)')\n",
    "_parser.add_argument('--scan', default=SCAN, help='name of scan to predict. If not specified, all valid scans are predicted')\n",
    "_parser.add_argument('--levels', default=PRED_LEVELS, type=int, help='zoom levels to work on for prediction')\n",
    "_parser.add_argument('--versions', default=VERSIONS, type=int, nargs='*', help='list of model versions to use. Must specify one for each level the in same order.')\n",
    "_parser.add_argument('--checkpoints', default=CHECKPOINTS, nargs='*', help='checkpoint to load for each version. None will load the last one.')\n",
    "_parser.add_argument('--norm-versions', default=NORM_VERSIONS, type=int, nargs='*', help='list of normalizer versionsto use. Must specify one for each level the in same order.')\n",
    "_parser.add_argument('--batch-size', default=BATCH_SIZE, type=int, help='batch size')\n",
    "_parser.add_argument('--size', default=SIZE, type=int, help='size of the image (as an integer, image is supposed square)')\n",
    "_parser.add_argument('--loss', default=LOSS, choices=['cross-entropy', 'mse', 'focal', 'bce'], help='loss function')\n",
    "_parser.add_argument('--csv', default=NORM_CSV, type=Path, help='path to csv where normalizer train images are stored')\n",
    "_parser.add_argument('--savedir', default=MODELS, type=Path, help='directory to save models and logs in')\n",
    "_parser.add_argument('--model', default=MODEL, help='name of the base architecture to use for classification')\n",
    "_parser.add_argument('--normalizer', default=NORMALIZER, help='encoder to use for normalization unet')\n",
    "_parser.add_argument('--norm-version', default=NORM_VERSION, type=int, help='version of the encoder to load for classification')\n",
    "_parser.add_argument('--rand-weights', action='store_true', help='specify to avoid using a pretrained model for training')\n",
    "_parser.add_argument('--gpus', default=GPUS, nargs='*', type=int, help='list of gpus you want to use for training (as numbers)')\n",
    "_parser.add_argument('--reduction', default=REDUCTION, choices=['mean', 'sum', 'none'], help='reduction to apply to loss')\n",
    "_parser.add_argument('--epochs', default=EPOCHS, type=int, help='number of epochs')\n",
    "_parser.add_argument('--weight', type=float, default=WEIGHT, help='weight to give to grade 1 (grade 3 being weighted to 1)')\n",
    "_parser.add_argument('--dropout', default=DROPOUT, type=float, help='dropout value')\n",
    "_parser.add_argument('--lr', default=LR, type=float, help='learning rate')\n",
    "_parser.add_argument('--wd', type=float, default=WD, help='weight decay')\n",
    "_parser.add_argument('--sample-mode', type=int, default=0, choices=[0, 1, 2], help='type 0 for regular sampling, 1 for oversampling, 2 for undersampling')\n",
    "_parser.add_argument('--transforms', type=int, default=TRANSFORMS, choices=[0, 1, 2, 3, 4], help='0 means no transform, above enables use of function `get_transformX` with X the number entered.')\n",
    "_parser.add_argument('--filt', default=FILT, choices=['K', 'K_inter', 'out', 'all', 'K_all'], help='patches to filter depending on their corresponding concept')\n",
    "_parser.add_argument('--open-mode', default=OPEN_MODE, choices=['3G', 'RGB'], help='How the image should be opened (3G for grayscale and RGB for color)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "hparams = _parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data.read.ipynb.\n",
      "Converted 11_data.loaders.ipynb.\n",
      "Converted 12_data.dataset.ipynb.\n",
      "Converted 13_data.utils.ipynb.\n",
      "Converted 14_data.transforms.ipynb.\n",
      "Converted 20_models.plmodules.ipynb.\n",
      "Converted 21_models.modules.ipynb.\n",
      "Converted 22_models.utils.ipynb.\n",
      "Converted 23_models.hooks.ipynb.\n",
      "Converted 24_models.metrics.ipynb.\n",
      "Converted 25_models.losses.ipynb.\n",
      "Converted 80_params.defaults.ipynb.\n",
      "Converted 81_params.parser.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
