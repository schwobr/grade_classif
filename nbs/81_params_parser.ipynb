{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp params.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from test_tube import HyperOptArgumentParser\n",
    "from grade_classif.params.defaults import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_parser = HyperOptArgumentParser(strategy='random_search')\n",
    "_parser.add_argument('--file', '-f', help='notebook convenience')\n",
    "_parser.add_argument('--HistoryManager.hist_file', help='nbconvert convenience')\n",
    "_parser.add_argument('--sched', default=SCHED, choices=['one-cycle', 'cosine-anneal', 'reduce-on-plateau', 'none'], help='scheduler for the optimizer')\n",
    "_parser.add_argument('--data', default=DATA, help='path to folder containing data')\n",
    "_parser.add_argument('--data-csv', default=DATA_CSV, help='path to csv listing scans with their grades and split')\n",
    "_parser.add_argument('--level', default=LEVEL, type=int, help='zoom level to work on')\n",
    "_parser.add_argument('--full-data', default=FULL_DATA, help='path to folder containing all data (different levels)')\n",
    "_parser.add_argument('--scan', default=SCAN, help='name of scan to predict. If not specified, all valid scans are predicted')\n",
    "_parser.add_argument('--levels', default=PRED_LEVELS, type=int, help='zoom levels to work on for prediction')\n",
    "_parser.add_argument('--versions', default=VERSIONS, type=int, nargs='*', help='list of model versions to use. Must specify one for each level the in same order.')\n",
    "_parser.add_argument('--norm-versions', default=NORM_VERSIONS, type=int, nargs='*', help='list of normalizer versionsto use. Must specify one for each level the in same order.')\n",
    "_parser.add_argument('--batch-size', default=BATCH_SIZE, type=int, help='batch size')\n",
    "_parser.add_argument('--size', default=SIZE, type=int, help='size of the image (as an integer, image is supposed square)')\n",
    "_parser.add_argument('--loss', default=LOSS, choices=['cross-entropy', 'mse'], help='loss function')\n",
    "_parser.add_argument('--csv', default=NORM_CSV, type=Path, help='path to csv where normalizer train images are stored')\n",
    "_parser.add_argument('--savedir', default=MODELS, type=Path, help='directory to save models and logs in')\n",
    "_parser.add_argument('--model', default=MODEL, choices=['resnet34', 'resnet50', 'efficientnet_b1'], help='name of the base architecture to use for classification')\n",
    "_parser.add_argument('--normalizer', default=NORMALIZER, help='encoder to use for normalization unet')\n",
    "_parser.add_argument('--norm-version', default=NORM_VERSION, type=int, help='version of the encoder to load for classification')\n",
    "_parser.add_argument('--rand-weights', action='store_true', help='specify to avoid using a pretrained model for training')\n",
    "_parser.add_argument('--gpus', default=GPUS, nargs='*', type=int, help='list of gpus you want to use for training (as numbers)')\n",
    "_parser.add_argument('--reduction', default=REDUCTION, choices=['mean', 'sum', 'none'], help='reduction to apply to loss')\n",
    "_parser.add_argument('--epochs', default=EPOCHS, type=int, help='number of epochs')\n",
    "_parser.opt_range('--weight', type=float, tunable=True, default=WEIGHT, low=1., high=10., nb_samples=8, help='weight to give to grade 1 (grade 3 being weighted to 1)')\n",
    "_parser.opt_range('--dropout', default=DROPOUT, type=float, tunable=True, low=0., high=0.8, nb_samples=5, help='dropout value')\n",
    "_parser.opt_range('--lr', default=LR, type=float, tunable=True, low=1e-4, high=1e-1, nb_samples=6, log_base=10, help='learning rate')\n",
    "_parser.opt_range('--wd', type=float, default=WD, tunable=True, low=1e-6, high=1., nb_samples=5, log_base=10, help='weight decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "hparams = _parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data_read.ipynb.\n",
      "Converted 11_data_loaders.ipynb.\n",
      "Converted 12_data_dataset.ipynb.\n",
      "Converted 13_data_utils.ipynb.\n",
      "Converted 14_data_transforms.ipynb.\n",
      "Converted 20_models_plmodules.ipynb.\n",
      "Converted 21_models_modules.ipynb.\n",
      "Converted 22_models_utils.ipynb.\n",
      "Converted 23_models_hooks.ipynb.\n",
      "Converted 24_models_metrics.ipynb.\n",
      "Converted 80_params_defaults.ipynb.\n",
      "Converted 81_params_parser.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
