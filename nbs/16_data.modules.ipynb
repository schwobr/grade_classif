{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pytorch_lightning as pl\n",
    "from grade_classif.imports import *\n",
    "from grade_classif.data.transforms import *\n",
    "from grade_classif.data.utils import show_img, LabelSlideBalancedRandomSampler\n",
    "from grade_classif.data.dataset import (\n",
    "    ImageClassifDataset,\n",
    "    MILDataset,\n",
    "    NormDataset,\n",
    "    RNNSlideDataset,\n",
    "    FeaturesClassifDataset\n",
    ")\n",
    "from grade_classif.core import ifnone\n",
    "from torch.utils.data import DataLoader, RandomSampler, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datafolder: Path,\n",
    "        data_csv: Path,\n",
    "        batch_size: int = 32,\n",
    "        size: int = 299,\n",
    "        transforms: Optional[int] = None,\n",
    "        patch_classes: Optional[Path] = None,\n",
    "        concepts: Optional[Path] = None,\n",
    "        concept_classes: Optional[Path] = None,\n",
    "        train_percent: float = 1,\n",
    "        get_id: Optional[Callable[[Any], str]] = None,\n",
    "        pacs_filt: Optional[str] = None,\n",
    "        num_workers: int = 4,\n",
    "        filterfile: Optional[str] = None,\n",
    "        val_fold: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.datafolder = datafolder\n",
    "        self.data_csv = data_csv\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_classes = patch_classes\n",
    "        self.concepts = concepts\n",
    "        self.concept_classes = concept_classes\n",
    "        self.size = size\n",
    "        self.transforms = transforms\n",
    "        self.train_percent = train_percent\n",
    "        self.get_id = get_id\n",
    "        self.pacs_filt = pacs_filt\n",
    "        self.num_workers = num_workers\n",
    "        self.filterfile = filterfile\n",
    "        self.val_fold = val_fold\n",
    "        if transforms is not None:\n",
    "            if transforms < 10:\n",
    "                tfm_func = globals()[f\"get_transforms{transforms}\"]\n",
    "                self.tfms = tfm_func(size)\n",
    "            else:\n",
    "                tfm_func = globals()[f\"get_transforms{10}\"]\n",
    "                self.tfms = tfm_func(size, n_inputs=transforms - 10)\n",
    "        else:\n",
    "            self.tfms = None\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return DataLoader(\n",
    "            self.data.valid, batch_size=self.batch_size, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        # OPTIONAL\n",
    "        # can also return a list of test dataloaders\n",
    "        if self.data.test is not None:\n",
    "            return DataLoader(\n",
    "                self.data.test,\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    \"\"\"def get_pacs_filt(self) -> Optional[Callable[[Path], bool]]:\n",
    "        if self.patch_classes is not None:\n",
    "            patch_classes_df = pd.read_csv(self.patch_classes, index_col=\"patchId\")\n",
    "            x_type = patch_classes_df.loc[x.stem, \"type\"]\n",
    "            if self.filt != \"all\":\n",
    "\n",
    "                def filt(x):\n",
    "                    return x_type == self.filt\n",
    "\n",
    "            else:\n",
    "\n",
    "                def filt(x):\n",
    "                    return x_type != \"garb\"\n",
    "\n",
    "        elif self.concepts is not None and self.concept_classes is not None:\n",
    "            conc_classes_df = pd.read_csv(self.concept_classes, index_col=0)\n",
    "            if self.filt != \"all\":\n",
    "                ok = conc_classes_df.loc[\n",
    "                    conc_classes_df[\"type\"] == self.filt\n",
    "                ].index.values\n",
    "            else:\n",
    "                ok = conc_classes_df.loc[conc_classes_df[\"type\"] != \"garb\"].index.values\n",
    "            conc_df = pd.read_csv(self.concepts, index_col=\"patchId\")\n",
    "\n",
    "            def filt(x):\n",
    "                return conc_df.loc[x.stem, \"concept\"] in ok\n",
    "\n",
    "        else:\n",
    "            filt = None\n",
    "        return filt\"\"\"\n",
    "    \n",
    "    def get_pacs_filt(self) -> Optional[Callable[[Path], bool]]:\n",
    "        if self.pacs_filt is None:\n",
    "            return None\n",
    "        else:\n",
    "            def filt(x):\n",
    "                for pacs in self.pacs_filt:\n",
    "                    if f\"PACS0{pacs}\" in x.name:\n",
    "                        return True\n",
    "                return False\n",
    "            return filt\n",
    "        \n",
    "    def get_accepted_files(self) -> Union[None, Set[str]]:\n",
    "        if self.filterfile is None:\n",
    "            return None\n",
    "        else:\n",
    "            with open(self.filterfile, \"r\") as f:\n",
    "                accepted_files = f.read().split(\"\\n\")\n",
    "            return set(accepted_files)\n",
    "\n",
    "    def show_some(self, n: int = 8, split: str = \"train\", imgsize: int = 4):\n",
    "        fig, axs = plt.subplots(n, 2, figsize=(imgsize * 2, imgsize * n))\n",
    "        data = getattr(self.data, split)\n",
    "        idxs = np.random.choice(np.arange(len(data)), size=n, replace=False)\n",
    "        for ax_r, idx in zip(axs, idxs):\n",
    "            x, x_tfmed = data.get_orig_tfmed(idx)\n",
    "            show_img(x, ax=ax_r[0])\n",
    "            show_img(x_tfmed, ax=ax_r[1])\n",
    "        title = \"original/transformed\"\n",
    "        fig.suptitle(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class NormDataModule(BaseDataModule):\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.data = (\n",
    "            NormDataset.from_folder(\n",
    "                self.datafolder,\n",
    "                extensions=[\".png\"],\n",
    "                train_percent=self.train_percent,\n",
    "                div=False,\n",
    "            )\n",
    "            .split_by_csv(self.data_csv, get_id=self.get_id, val_fold=self.val_fold)\n",
    "            .to_tensor(tfms=self.tfms)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageClassifDataModule(BaseDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datafolder: Path,\n",
    "        data_csv: Path,\n",
    "        classes: Sequence[str],\n",
    "        label_func: Callable[[Path], str],\n",
    "        sample_mode: int = 0,\n",
    "        max_patches_per_slide: Optional[int] = None,\n",
    "        norm_ref : Optional[str] = None,\n",
    "        norm_method : Optional[str] = None,\n",
    "        include : Optional[Sequence[str]] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(datafolder, data_csv, **kwargs)\n",
    "        self.classes = classes\n",
    "        self.n_classes = len(classes)\n",
    "        self.sample_mode = sample_mode\n",
    "        self.label_func = label_func\n",
    "        self.max_patches_per_slide = max_patches_per_slide\n",
    "        self.norm_ref = norm_ref\n",
    "        self.norm_method = norm_method\n",
    "        self.include=include\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if not hasattr(self, \"data\"):\n",
    "            self.filt = self.get_pacs_filt()\n",
    "            data = ImageClassifDataset.from_folder(\n",
    "                self.datafolder,\n",
    "                self.label_func,\n",
    "                classes=self.classes,\n",
    "                extensions=[\".png\"],\n",
    "                include=self.include,\n",
    "                filterfunc=self.filt,\n",
    "                accepted_files=self.get_accepted_files(),\n",
    "                train_percent=self.train_percent,\n",
    "                norm_ref = self.norm_ref,\n",
    "                norm_method = self.norm_method,\n",
    "                div=False\n",
    "            ).split_by_csv(self.data_csv, get_id=self.get_id, val_fold=self.val_fold)\n",
    "            self.data = data.to_tensor(tfms=self.tfms, tfm_y=False)\n",
    "\n",
    "    def get_slide_weights(self) -> NDArray[(Any,), float]:\n",
    "        items = self.data.train.items\n",
    "        get_slide = np.vectorize(lambda x: \"_\".join(x.name.split(\"_\")[:-2]))\n",
    "        all_slides = get_slide(items)\n",
    "        slides = np.unique(all_slides)\n",
    "        n = len(items)\n",
    "        weights = np.ones_like(items, dtype=np.float32)\n",
    "        n_patches_max = 0\n",
    "        for slide in slides:\n",
    "            slide_mask = all_slides == slide\n",
    "            n_patches = slide_mask.sum()\n",
    "            weight = n / n_patches\n",
    "            weights[slide_mask] = weight\n",
    "            if self.sample_mode == 1:\n",
    "                n_patches_max += self.max_patches_per_slide\n",
    "            else:\n",
    "                n_patches_max += min(self.max_patches_per_slide, n_patches)\n",
    "        return weights, n_patches_max\n",
    "\n",
    "    def get_n_patches_max(self, all_slides: NDArray[(Any,), str]) -> int:\n",
    "        slides = np.unique(all_slides)\n",
    "        if self.sample_mode == 1:\n",
    "            return len(slides) * self.max_patches_per_slide\n",
    "        else:\n",
    "            n_patches_max = 0\n",
    "            for slide in slides:\n",
    "                n_patches = (all_slides == slide).sum()\n",
    "                n_patches_max += min(n_patches, self.max_patches_per_slide)\n",
    "            return n_patches_max\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        sm = self.sample_mode\n",
    "        labels = self.data.train.labels\n",
    "        items = self.data.train.items\n",
    "        get_slide = np.vectorize(lambda x: \"_\".join(x.name.split(\"_\")[:-2]))\n",
    "        all_slides = get_slide(items)\n",
    "        if sm > 0:\n",
    "            if self.max_patches_per_slide is not None:\n",
    "                n_patches_max = self.get_n_patches_max(all_slides)\n",
    "            else:\n",
    "                n_patches_max = len(labels)\n",
    "            if sm == 1:\n",
    "                n = max([(labels == cl).sum() for cl in self.classes])\n",
    "                replacement = True\n",
    "            else:\n",
    "                n = min([(labels == cl).sum() for cl in self.classes])\n",
    "                replacement = False\n",
    "            n_patches = int(self.n_classes * n * n_patches_max / len(labels))\n",
    "            sampler = LabelSlideBalancedRandomSampler(\n",
    "                labels, all_slides, n_patches, replacement=replacement\n",
    "            )\n",
    "        else:\n",
    "            sampler = RandomSampler(self.data.train)\n",
    "        return DataLoader(\n",
    "            self.data.train,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FeaturesClassifDataModule(BaseDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datafolder: Path,\n",
    "        data_csv: Path,\n",
    "        classes: Sequence[str],\n",
    "        label_func: Callable[[Path], str],\n",
    "        sample_mode: int = 0,\n",
    "        embed_dim: int = 2048,\n",
    "        padding_file: Optional[Path] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(datafolder, data_csv, **kwargs)\n",
    "        self.classes = classes\n",
    "        self.n_classes = len(classes)\n",
    "        self.sample_mode = sample_mode\n",
    "        self.label_func = label_func\n",
    "        self.embed_dim = embed_dim\n",
    "        self.padding_file = padding_file\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if not hasattr(self, \"data\"):\n",
    "            self.filt = self.get_pacs_filt()\n",
    "            data = FeaturesClassifDataset.from_folder(\n",
    "                self.datafolder,\n",
    "                self.label_func,\n",
    "                classes=self.classes,\n",
    "                extensions=[\".csv\"],\n",
    "                include=self.classes,\n",
    "                filterfunc=self.filt,\n",
    "                train_percent=self.train_percent,\n",
    "                size=self.size,\n",
    "                embed_dim=self.embed_dim,\n",
    "                padding_file = self.padding_file\n",
    "            ).split_by_csv(self.data_csv, get_id=self.get_id, val_fold=self.val_fold)\n",
    "            self.data = data.to_tensor()\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        sm = self.sample_mode\n",
    "        labels = self.data.train.labels\n",
    "        if sm > 0:\n",
    "            weights = torch.zeros(len(labels), dtype=torch.float32)\n",
    "            for k, cl in enumerate(self.classes):\n",
    "                mask = labels == cl\n",
    "                weights[mask] = len(labels) / mask.sum()\n",
    "            if sm == 1:\n",
    "                n = max([(labels == cl).sum() for cl in self.classes])\n",
    "            else:\n",
    "                n = min([(labels == cl).sum() for cl in self.classes])\n",
    "            n_patches = int(self.n_classes * n)\n",
    "            sampler = WeightedRandomSampler(weights, n_patches, replacement=True)\n",
    "        else:\n",
    "            sampler = RandomSampler(self.data.train)\n",
    "        return DataLoader(\n",
    "            self.data.train,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MILDataModule(BaseDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datafolder: Union[str, Path],\n",
    "        data_csv: Union[str, Path],\n",
    "        coord_csv: Union[str, Path],\n",
    "        classes: Sequence[str],\n",
    "        label_func: Optional[Callable[[Path], str]] = None,\n",
    "        extensions: Optional[Sequence[str]] = None,\n",
    "        level: int = 1,\n",
    "        sample_mode: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(datafolder, data_csv, **kwargs)\n",
    "        assert len(classes) == 2, \"MIL only works with 2 classes.\"\n",
    "        self.sample_mode = sample_mode\n",
    "        self.coord_csv = coord_csv\n",
    "        self.classes = classes\n",
    "        self.label_func = ifnone(label_func, lambda x: x.parent.name)\n",
    "        self.extensions = ifnone(extensions, [\".mrxs\"])\n",
    "        self.level = level\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.data = (\n",
    "            MILDataset.from_folder(\n",
    "                self.datafolder,\n",
    "                self.label_func,\n",
    "                self.coord_csv,\n",
    "                classes=self.classes,\n",
    "                extensions=self.extensions,\n",
    "                include=self.classes,\n",
    "                train_percent=self.train_percent,\n",
    "                size=self.size,\n",
    "                level=self.level,\n",
    "            )\n",
    "            .split_by_csv(self.data_csv)\n",
    "            .to_tensor(tfms=self.tfms, tfm_y=False)\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        sm = self.sample_mode\n",
    "        w = np.float32(\n",
    "            (self.data.train.labels == self.classes[0]).sum()\n",
    "            / (self.data.train.labels == self.classes[1]).sum()\n",
    "        )\n",
    "        min_class = 1\n",
    "        if w < 1:\n",
    "            w = 1 / w\n",
    "            min_class = 0\n",
    "        if sm > 0:\n",
    "            labels = self.data.train.labels == self.classes[min_class]\n",
    "            weights = np.where(labels, w, 1.0)\n",
    "            if sm == 1:\n",
    "                sampler = WeightedRandomSampler(weights, 2 * len(np.argwhere(~labels)))\n",
    "            else:\n",
    "                sampler = WeightedRandomSampler(\n",
    "                    weights, 2 * len(np.argwhere(labels))\n",
    "                )\n",
    "        else:\n",
    "            sampler = RandomSampler(self.data.train)\n",
    "        return DataLoader(\n",
    "            self.data.train,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            drop_last=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return DataLoader(\n",
    "            self.data.valid,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data.test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RNNAggDataModule(MILDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datafolder: Union[str, Path],\n",
    "        data_csv: Union[str, Path],\n",
    "        coord_csv: Union[str, Path],\n",
    "        classes: Sequence[str],\n",
    "        patches_per_slide=10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(datafolder, data_csv, coord_csv, classes, **kwargs)\n",
    "        self.patches_per_slide = patches_per_slide\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.data = (\n",
    "            RNNSlideDataset.from_folder(\n",
    "                self.datafolder,\n",
    "                self.label_func,\n",
    "                self.coord_csv,\n",
    "                classes=self.classes,\n",
    "                extensions=self.extensions,\n",
    "                include=self.classes,\n",
    "                train_percent=self.train_percent,\n",
    "                size=self.size,\n",
    "                level=self.level,\n",
    "                patches_per_slide=self.patches_per_slide,\n",
    "            )\n",
    "            .split_by_csv(self.data_csv)\n",
    "            .to_tensor(tfms=self.tfms, tfm_y=False)\n",
    "        )\n",
    "        weights = np.float32(\n",
    "            (self.data.train.labels == self.classes[0]).sum()\n",
    "            / (self.data.train.labels == self.classes[1]).sum()\n",
    "        )\n",
    "        min_class = 1\n",
    "        if weights < 1:\n",
    "            weights = 1 / weights\n",
    "            min_class = 0\n",
    "        self.min_class = min_class\n",
    "        self.weights = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data.read.ipynb.\n",
      "Converted 11_data.loaders.ipynb.\n",
      "Converted 12_data.dataset.ipynb.\n",
      "Converted 13_data.utils.ipynb.\n",
      "Converted 14_data.transforms.ipynb.\n",
      "Converted 15_data.color.ipynb.\n",
      "Converted 16_data.modules.ipynb.\n",
      "Converted 20_models.plmodules.ipynb.\n",
      "Converted 21_models.modules.ipynb.\n",
      "Converted 22_models.utils.ipynb.\n",
      "Converted 23_models.hooks.ipynb.\n",
      "Converted 24_models.metrics.ipynb.\n",
      "Converted 25_models.losses.ipynb.\n",
      "Converted 80_params.defaults.ipynb.\n",
      "Converted 81_params.parser.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
