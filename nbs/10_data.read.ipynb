{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data \n",
    "> Defines utilities function for reading and manipulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from grade_classif.params.defaults import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from grade_classif.core import ifnone\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _check_include(obj, include):\n",
    "    return include is None or obj.name in include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _check_exclude(obj, exclude):\n",
    "    return exclude is None or obj.name not in exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _check_valid(obj, include, exclude):\n",
    "    return _check_include(obj, include) and _check_exclude(obj, exclude) and not obj.name.startswith('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_items(folder, label_func, recurse=True, extensions=None, include=None, exclude=None, filterfunc=None):\n",
    "    items = []\n",
    "    labels = []\n",
    "    filterfunc = ifnone(filterfunc, lambda x: True)    \n",
    "    for obj in folder.iterdir():\n",
    "        if obj.is_file():\n",
    "            if extensions is None or obj.suffix in extensions and filterfunc(obj):\n",
    "                items.append(obj)\n",
    "                labels.append(label_func(obj))\n",
    "        elif recurse and _check_valid(obj, include, exclude):\n",
    "            items_r, labels_r = get_items(obj, label_func, extensions=extensions, filterfunc=filterfunc)\n",
    "            items += items_r\n",
    "            labels += labels_r\n",
    "    return items, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads all items and labels in `folder`. Items are stored as `Path` objects. Labels are computed for each item using `label_func`. By default, the search will happen recursively in all subfolders. To disable this behaviour, use `recurse=False`. You can also specify a list of `extensions` to restrict the accepted files, as well as `include` and `exclude` folders (as `str`, these are direct subfolders of `folder`). `filterfunc` can be used to only accept objects for which it returns `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_scan(folder, scan_name, include=None, exclude=None):\n",
    "    dirs = []\n",
    "    for item in folder.iterdir():\n",
    "        if item.name == scan_name and _check_valid(item, include, exclude):\n",
    "            return item\n",
    "        if item.is_dir():\n",
    "            dirs.append(item)\n",
    "    for item in dirs:\n",
    "        obj = get_scan(item, scan_name)\n",
    "        if obj is not None:\n",
    "            return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a `Path` object to the folder corresponding to a `scan` name in a specific `folder`. Direct subfolders can be include or excluded using respectively `include` and `exclude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/work/stages/schwob/Patches_240/Patches_MGI_240_2/3/CF_PACS04rescan_10F0051')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scan(DATA, 'CF_PACS04rescan_10F0051', include=['1', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def split(scans, grades, valid_pct=0.2):\n",
    "    grades1 = list(filter(lambda x: x == '1', grades))\n",
    "    order = np.random.permutation(len(scans))\n",
    "    n = {'1': len(grades1), '3': len(grades)-len(grades1)}\n",
    "    k = {'1': 0, '3': 0}\n",
    "    splits = ['' for _ in scans]\n",
    "    for o in order:\n",
    "        grade, scan = grades[o], scans[o]\n",
    "        if k[grade] > valid_pct*n[grade]:\n",
    "            split = 'train'\n",
    "        else:\n",
    "            split = 'valid'\n",
    "        k[grade] += 1\n",
    "        splits[o] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a list of scan names `scans`,a list of grades `grades` (both as strings) and ratio for the validation set's size `valid_pct`, splits the dataset between training and validation sets. Returns a list of `'train'` and `'valid'` strings so that each scan is associated to a set. Lists are reordered inside the function so that the splitting is random. Proportions of grades 1 and 3 are preserved in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _remove_doubles(scans, grades):\n",
    "    scans_res = []\n",
    "    grades_res = []\n",
    "    for scan, grade in zip(scans, grades):\n",
    "        if scan not in scans_res:\n",
    "            scans_res.append(scan)\n",
    "            grades_res.append(grade)\n",
    "    return scans_res, grades_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_csv(csv_path, data_path, label_func=None):\n",
    "    label_func = ifnone(label_func, lambda x: x.parts[-3])\n",
    "    scans, grades = get_items(data_path, label_func, include=['1', '3'])\n",
    "    scans = list(map(lambda x: x.parent.name, scans))\n",
    "    scans, grades = _remove_doubles(scans, grades)\n",
    "    splits = split(scans, grades)\n",
    "    df = pd.DataFrame({'scan': scans, 'grade': grades, 'split': splits})\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a csv that contains all scan names, with the associated grade and subset they are in. Scans are found in the folder contained in `data_path` (`Path` object). Doubles are deleted. The file is then stored in `csv_path`. You can specify a `label_func` to compute how the grade is to be extracted from the path to a patch. Returns the corresponding dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_train.ipynb.\n",
      "Converted 02_predict.ipynb.\n",
      "Converted 10_data_read.ipynb.\n",
      "Converted 11_data_loaders.ipynb.\n",
      "Converted 12_data_dataset.ipynb.\n",
      "Converted 13_data_utils.ipynb.\n",
      "Converted 14_data_transforms.ipynb.\n",
      "Converted 20_models_plmodules.ipynb.\n",
      "Converted 21_models_modules.ipynb.\n",
      "Converted 22_models_utils.ipynb.\n",
      "Converted 23_models_hooks.ipynb.\n",
      "Converted 24_models_metrics.ipynb.\n",
      "Converted 80_params_defaults.ipynb.\n",
      "Converted 81_params_parser.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
