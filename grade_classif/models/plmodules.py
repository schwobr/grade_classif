#AUTOGENERATED! DO NOT EDIT! File to edit: dev/20_models.plmodules.ipynb (unless otherwise specified).

__all__ = ['BaseModule', 'Normalizer', 'GradesClassifModel', 'RNNAttention']

#Cell
from ..data.dataset import ImageClassifDataset, NormDataset
from ..data.transforms import *
from ..data.utils import show_img
from .utils import named_leaf_modules, get_sizes, get_num_features, gaussian_mask
from .modules import *
from .losses import FocalLoss, BCE
from ..core import ifnone
from ..imports import *
import pytorch_lightning as pl
from pytorch_lightning.logging import CometLogger
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.utils.data import DataLoader, RandomSampler, WeightedRandomSampler
from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau

#Cell
def _get_loss(loss_name, weight, reduction, device='cpu'):
    if loss_name == 'cross-entropy':
        loss = nn.CrossEntropyLoss(torch.tensor([weight, 1.], device=device), reduction=reduction)
    elif loss_name == 'bce':
        loss = BCE(reduction=reduction, pos_weight=torch.tensor([1/weight], device=device))
    elif loss_name == 'mse':
        loss = nn.MSELoss(reduction=reduction)
    elif loss_name == 'focal':
        loss = FocalLoss(reduction=reduction)
    return loss.__call__

#Cell
def _get_scheduler(opt, name, total_steps, lr):
    if name == 'one-cycle':
        sched = OneCycleLR(opt, lr, total_steps=total_steps)
        sched.step_on_batch = True
    elif name == 'cosine-anneal':
        sched = CosineAnnealingLR(opt, total_steps)
        sched.step_on_batch = True
    elif name == 'reduce-on-plateau':
        sched= ReduceLROnPlateau(opt)
        sched.step_on_batch = False
    else:
        sched = None
    return sched

#Cell
class BaseModule(pl.LightningModule):
    """
    """
    def __init__(self, hparams, metrics=None):
        super().__init__()
        self.hparams = hparams
        self.main_device = 'cpu' if hparams.gpus is None else f'cuda:{hparams.gpus[0]}'
        # self.main_device = 'cuda:1'
        try:
            weight = hparams.weight if hparams.sample_mode == 0 else 1.
        except AttributeError:
            weight = 1.
        self.loss = _get_loss(hparams.loss, weight, hparams.reduction, device=self.main_device)
        self.bs = hparams.batch_size
        self.lr = hparams.lr
        self.wd = hparams.wd
        self.metrics = ifnone(metrics, [])
        model_type = 'normalizer' if isinstance(self, Normalizer) else 'classifier'
        self.save_path = hparams.savedir/f'level_{hparams.level}/{model_type}/{hparams.model if model_type == "classifier" else hparams.normalizer}'

    def post_init(self):
        self.leaf_modules = named_leaf_modules(self)
        self.sizes, self.leaf_modules = get_sizes(self, input_shape=(3, self.hparams.size, self.hparams.size), leaf_modules=self.leaf_modules)
        self = self.to(self.main_device)

    def on_train_start(self):
        self.train()

    def training_step(self, batch, batch_nb):
        # REQUIRED
        x, y = batch
        y_hat = self(x)
        loss = self.loss(y_hat, y)
        lr = self.sched.optimizer.param_groups[-1]['lr']
        log = {'train_loss': loss, 'learning_rate': lr}
        return {'loss': loss, 'log': log}


    def validation_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        y_hat = self(x)
        loss = self.loss(y_hat, y)
        return {'val_loss': loss}


    def validation_end(self, outputs):
        # OPTIONAL
        loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        log = {'val_loss': loss}
        return {'val_loss': loss, 'log': log}


    def test_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        y_hat = self(x)
        return {'test_loss': self.loss(y_hat, y)}


    def test_end(self, outputs):
        # OPTIONAL
        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
        return {'avg_test_loss': avg_loss}

    def configure_optimizers(self):
        # REQUIRED
        self.opt = torch.optim.Adam(self.parameters(), lr=self.lr)
        self.sched = _get_scheduler(self.opt, self.hparams.sched, self.hparams.epochs*len(self.train_dataloader()), self.lr)
        return opt

    def on_after_backward(self):
        for pg in self.opt.param_groups:
            for p in pg['params']: p.data.mul_(1 - self.wd*pg['lr'])

    def on_batch_end(self):
        if self.sched is not None and self.sched.step_on_batch:
            self.sched.step()

    def on_epoch_end(self):
        if self.sched is not None and not self.sched.step_on_batch:
            self.sched.step()

    @pl.data_loader
    def train_dataloader(self):
        return DataLoader(self.data.train, batch_size=self.bs, shuffle=True, drop_last=True)


    @pl.data_loader
    def val_dataloader(self):
        # OPTIONAL
        # can also return a list of val dataloaders
        return DataLoader(self.data.valid, batch_size=self.bs)

    @pl.data_loader
    def test_dataloader(self):
        # OPTIONAL
        # can also return a list of test dataloaders
        return DataLoader(self.data.test, batch_size=self.bs) if self.data.test is not None else None

    def load(self, version, ckpt_epoch=None):
        """
        Load a specific `version` of current model, stored in `self.save_path/lightning_logs`. If multiple
        checkpoints have been stored, `ckpt_epoch` can be specified to load a specific epoch. Else the latest
        epoch is loaded.
        """
        save_dir = self.save_path/f'lightning_logs/version_{version}/checkpoints'
        path = list(save_dir.iterdir())[-1] if ckpt_epoch is None else save_dir/f'_ckpt_epoch_{ckpt_epoch}.ckpt'
        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)
        self.load_state_dict(checkpoint['state_dict'])

    def my_summarize(self):
        """
        Get a DataFrame containing the list of all leaf modules of current model, with their corresponding output
        shape.
        """
        summary = pd.DataFrame({'Name': list(map(lambda x: x.name, self.leaf_modules)), 'Output Shape': self.sizes})
        return summary

    def fit(self, **kwargs):
        """
        Fit the model using parameters stored in `hparams`.
        """
        logger = CometLogger(api_key=os.environ['COMET_API_KEY'], workspace='schwobr', save_dir=self.save_path, project_name='grade-classif')
        logger.experiment.add_tag('norm' if isinstance(self, Normalizer) else 'classif')
        ckpt_path = self.save_path/'lightning_logs'/f'version_{logger.version}'/'checkpoints'
        ckpt_callback = ModelCheckpoint(ckpt_path, save_top_k=3)
        trainer = pl.Trainer(gpus=self.hparams.gpus, checkpoint_callback=ckpt_callback, logger=logger, min_epochs=self.hparams.epochs, max_epochs=self.hparams.epochs, **kwargs)
        self.version = trainer.logger.version
        trainer.fit(self)

    def predict(self, x):
        """
        Make a prediction on batch `x`.
        """
        return self.eval()(x)

    def show_some(self, n=8, split='train', imgsize=4):
        fig, axs = plt.subplots(n, 2, figsize=(imgsize*2, imgsize*n))
        data = getattr(self.data, split)
        idxs = np.random.choice(np.arange(len(data)), size=n, replace=False)
        for ax_r, idx in zip(axs, idxs):
            x, x_tfmed = data.get_orig_tfmed(idx)
            show_img(x, ax=ax_r[0])
            show_img(x_tfmed, ax=ax_r[1])
        title = 'original/transformed'
        fig.suptitle(title)
        plt.show()

#Cell
class Normalizer(BaseModule):
    """
    """
    def __init__(self, hparams, **kwargs):
        super().__init__(hparams, **kwargs)
        input_shape = (3, hparams.size, hparams.size)
        self.unet = DynamicUnet(hparams.normalizer, n_classes=3, input_shape=input_shape, pretrained=not hparams.rand_weights)

        data = (NormDataset.
                from_folder(Path(hparams.data), extensions=['.png'], open_mode=hparams.open_mode).
                split_by_csv(hparams.data_csv))

        if hparams.transforms:
            tfms = globals()[f'get_transforms{hparams.transforms}'](hparams.size, num_els=len(data.valid))
        else:
            tfms = []

        self.data = data.to_tensor(tfms=tfms)
        self.post_init()

    def on_epoch_start(self):
        for tfm in self.data.valid.tfms:
            if 'Deterministic' in type(tfm):
                tfm.n = 0


    def forward(self, x):
        return self.unet(x)


    def show_results(self, n=16, imgsize=4, title=None):
        """
        Plot `n` predictions from the normalizer. Each line will contain input, target and prediction
        images (in that order).
        """
        n = min(n, self.bs)
        fig, axs = plt.subplots(n, 3, figsize=(imgsize*3, imgsize*n))
        idxs = np.random.choice(np.arange(len(self.data.valid)), size=n, replace=False)
        inputs = []
        targs = []
        for idx in idxs:
            x, y = self.data.valid[idx]
            inputs.append(x)
            targs.append(y)
        inputs = torch.stack(inputs).to(next(self.parameters()).device)
        preds = self.predict(inputs).clamp(0, 1)
        for ax_r, x, y, z in zip(axs, inputs, targs, preds):
            x = x.cpu().numpy().transpose(1, 2, 0)
            y = y.numpy().transpose(1, 2, 0)
            z = z.detach().cpu().numpy().transpose(1, 2, 0)
            show_img(x, ax=ax_r[0])
            show_img(y, ax=ax_r[1])
            show_img(z, ax=ax_r[2])
        title = ifnone(title, 'input/target/prediction')
        fig.suptitle(title)
        plt.show()

    def freeze_encoder(self):
        """
        Freeze the encoder part of the normalizer.
        """
        for m in self.leaf_modules:
            if 'encoder' in m.name and not isinstance(m, nn.BatchNorm2d):
                for param in m.parameters():
                    param.requires_grad = False

    def init_bn(self):
        """
        Initialize BatchNorm layers with bias `1e-3` and weights `1`.
        """
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d):
                with torch.no_grad():
                    m.bias.fill_(1e-3)
                    m.weight.fill_(1.)

#Cell
class GradesClassifModel(BaseModule):
    """
    """
    def __init__(self, hparams, **kwargs):
        super().__init__(hparams, **kwargs)

        if hparams.transforms:
            tfms = globals()[f'get_transforms{hparams.transforms}'](hparams.size)
        else:
            tfms = []

        if hparams.patch_classes is not None:
            patch_classes_df = pd.read_csv(hparams.patch_classes, index_col='patchId')
            if hparams.filt != 'all':
                def filt(x):
                    return patch_classes_df.loc[x.stem, 'type'] == hparams.filt
            else:
                def filt(x):
                    return patch_classes_df.loc[x.stem, 'type'] != 'garb'
        elif hparams.concepts is not None and hparams.concept_classes is not None:
            conc_classes_df = pd.read_csv(hparams.concept_classes, index_col=0)
            if hparams.filt != 'all':
                ok = conc_classes_df.loc[conc_classes_df['type'] == hparams.filt].index.values
            else:
                ok = conc_classes_df.loc[conc_classes_df['type'] != 'garb'].index.values
            conc_df = pd.read_csv(hparams.concepts, index_col='patchId')
            def filt(x):
                return conc_df.loc[x.stem, 'concept'] in ok
        else:
            filt = None

        self.filt = filt
        self.data = (ImageClassifDataset.
                     from_folder(Path(hparams.data), lambda x: x.parts[-3], classes=['1', '3'], extensions=['.png'], include=['1', '3'], open_mode=hparams.open_mode, filterfunc=filt).
                     split_by_csv(hparams.data_csv).
                     to_tensor(tfms=tfms, tfm_y=False))
        weight = np.float32((self.data.train.labels == '3').sum()/(self.data.train.labels == '1').sum())
        self.hparams.weight = weight
        self.loss = _get_loss(hparams.loss, weight if hparams.sample_mode == 0 else 1., hparams.reduction, device=self.main_device)

        if 'cbr' in hparams.model:
            args = map(int, hparams.model.split('_')[1:])
            base_model = CBR(*args)
            cut = -3
        elif 'sasa' in hparams.model:
            args = map(int, hparams.model.split('_')[1:])
            base_model = SASA(*args)
            cut = -3
        elif 'sanet' in hparams.model:
            splits = hparams.model.split('_')
            kernel_size = int(splits[-1])
            base_model = globals()[splits[0]](kernel_size)
            cut = -2
        else:
            base_model = timm.create_model(hparams.model, pretrained=not hparams.rand_weights)
            cut = -2
        self.base_model = nn.Sequential(*list(base_model.children())[:cut])
        head = [nn.AdaptiveAvgPool2d(1), nn.Flatten()]
        nf = get_num_features(self.base_model)
        p = hparams.dropout
        nc = 2 if hparams.loss == 'cross-entropy' else 1
        head += bn_drop_lin(nf, nf, p=p/2) + bn_drop_lin(nf, nc, p=p)
        self.head = nn.Sequential(*head)
        self.post_init()
        self._create_normalizer()

    @pl.data_loader
    def train_dataloader(self):
        sm = self.hparams.sample_mode
        w = self.hparams.weight
        if sm > 0:
            labels = self.data.train.labels == '1'
            weights = np.where(labels, w, 1.)
            if sm == 1:
                sampler = WeightedRandomSampler(weights, 2*len(np.argwhere(~labels)))
            else:
                # sampler = WeightedRandomSampler(weights, 2*len(np.argwhere(labels)), replacement=False)
                sampler = WeightedRandomSampler(weights, 40000, replacement=False)
        else:
            sampler = RandomSampler(self.data.train)
        return DataLoader(self.data.train, batch_size=self.bs, sampler=sampler, drop_last=True)

    def validation_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        y_hat = self(x)
        loss = self.loss(y_hat, y)
        ret = {'val_loss': loss}
        n = y.shape[0]
        if self.hparams.loss == 'cross-entropy':
            y_hat = torch.softmax(y_hat, dim=1)
            y_hat = y_hat.argmax(dim=-1).view(n,-1)
        else:
            y_hat = torch.sigmoid(y_hat)
            y_hat = (y_hat > 0.5).view(n, -1)
        y = y.view(n,-1)
        ret['tp'] = ((y_hat)&(y==1)).float().sum()
        ret['tn'] = ((~y_hat)&(y==0)).float().sum()
        ret['fp'] = ((y_hat)&(y==0)).float().sum()
        ret['fn'] = ((~y_hat)&(y==1)).float().sum()
        return ret

    def validation_end(self, outputs):
        # OPTIONAL
        loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        log = {'val_loss': loss}
        tp = torch.stack([x['tp'] for x in outputs]).sum()
        fp = torch.stack([x['fp'] for x in outputs]).sum()
        tn = torch.stack([x['tn'] for x in outputs]).sum()
        fn = torch.stack([x['fn'] for x in outputs]).sum()
        for metric in self.metrics:
            try:
                name = metric.__name__
            except AttributeError:
                name = metric.func.__name__
                kws = metric.keywords
                for k in kws:
                    name += f'_{k}_{kws[k]}'
            log[name] = metric(tp, fp, tn, fn)
        return {'val_loss': loss, 'log': log}

    def _create_normalizer(self):
        hparams = self.hparams
        if hparams.normalizer is not None:
            norm = DynamicUnet(hparams.normalizer, n_classes=3, input_shape=(3, hparams.size, hparams.size), pretrained=True)
            if hparams.norm_version is not None:
                save_dir = self.save_path.parents[1]/'normalizer'/f'{hparams.normalizer}/lightning_logs/version_{hparams.norm_version}/checkpoints'
                path = next(save_dir.iterdir())
                checkpoint = torch.load(path, map_location=lambda storage, loc: storage)
                state_dict = {}
                for k in checkpoint['state_dict']:
                    state_dict[k.replace('unet.', '')] = checkpoint['state_dict'][k]
                norm.load_state_dict(state_dict)
                for p in norm.parameters():
                    p.requires_grad = False
            norm = norm.to(self.main_device)
            self.norm = norm.__call__

    def forward(self, x):
        if hasattr(self, 'norm'):
            x = self.norm(x)
        x = self.base_model(x)
        x = self.head(x)
        return x

    def predict(self, x):
        pred = super().predict(x)
        if self.hparams.loss == 'cross-entropy':
            pred = torch.softmax(pred, dim=1)
        else:
            pred = torch.sigmoid(pred)
        return pred

    def freeze_base(self):
        """
        Freeze the base model.
        """
        for m in self.leaf_modules:
            if 'base_model' in m.name and not isinstance(m, nn.BatchNorm2d):
                for param in m.parameters():
                    param.requires_grad = False

#Cell
class RNNAttention(BaseModule):
    """
    """
    def __init__(self, hparams, **kwargs):
        super().__init__(hparams, **kwargs)

        if hparams.transforms:
            tfms = globals()[f'get_transforms{hparams.transforms}'](hparams.size)
        else:
            tfms = []

        if hparams.concepts is not None and hparams.concept_classes is not None:
            conc_classes_df = pd.read_csv(hparams.concept_classes, index_col=0)
            if hparams.filt != 'all':
                ok = conc_classes_df.loc[conc_classes_df['type'] == hparams.filt].index.values
            else:
                ok = conc_classes_df.loc[conc_classes_df['type'] != 'garb'].index.values
            conc_df = pd.read_csv(hparams.concepts, index_col='patchId')
            def filt(x):
                return conc_df.loc[x.stem, 'concept'] in ok
        else:
            filt = None

        self.data = (ImageClassifDataset.
                     from_folder(Path(hparams.data), lambda x: x.parts[-3], classes=['1', '3'], extensions=['.png'], include=['1', '3'], open_mode=hparams.open_mode, filterfunc=filt).
                     split_by_csv(hparams.data_csv).
                     to_tensor(tfms=tfms, tfm_y=False))

        weight = np.float32((self.data.train.labels == '3').sum()/(self.data.train.labels == '1').sum())
        self.hparams.weight = weight
        self.loss = _get_loss(hparams.loss, weight if hparams.sample_mode == 0 else 1., hparams.reduction, device=self.main_device)

        nc = 2 if hparams.loss == 'cross-entropy' else 1
        self.t_x = nn.Sequential(*list(CBR(3, 64, 2).children())[:-1])
        nf = get_num_features(self.t_x)
        self.fc = nn.Linear(nx, nc)
        self.t_l = nn.Sequential(nn.Linear(6, nf),
                                 nn.ReLU(),
                                 nn.Linear(nf, 2*nf),
                                 nn.ReLU(),
                                 nn.Linear(2*nf, nf))
        self.t_a = nn.Sequential(nn.Linear(nf, 2*nf),
                                 nn.ReLU(),
                                 nn.Linear(2*nf, nf),
                                 nn.ReLU(),
                                 nn.Linear(nf, 6))
        self.final_head = nn.Sequential(nn.Linear(nx*self.hparams.n_glimpses, nx),
                                        nn.ReLU(),
                                        nn.Linear(nf, nc))


    def forward(self, X, l):
        Ai = gaussian_mask(*l[:3], self.hparams.glimpse_size, self.hparams.size)
        Aj = gaussian_mask(*l[3:], self.hparams.glimpse_size, self.hparams.size)
        x = Ai[None, None] @ X @ Aj.T[None, None]
        if hasattr(self, 'norm'):
            x = self.norm(x)
        fx = self.t_x(x)
        y = self.fc(fx)
        fl = self.t_l(l)
        l = torch.sigmoid(fx * fl)
        l = self.t_a(l)
        return fx, y, l

    def compute_loss(self, X, Y):
        """
        Compute the loss defined in the paper. Return the final slide-level prediction as well.
        """
        l0 = torch.tensor([0, .5, self.hparams.size//self.hparams.glimpse_size]*2, device=self.main_device)
        loss = 0
        loss_prev = 0
        preds = []
        for t in range(self.hparams.n_glimpses):
            fx, y_hat, l = self(X, l0)
            fts.append(fx)
            loss += self.loss(y_hat, Y)
            loss_a = (y_hat**2).sum()
            if t > 0:
                loss -= loss_a - loss_prev / t
            loss_prev += loss_a
            loss += self.hparams.gamma * torch.exp(-torch.abs(l-l0))
            l0 = l.detach()
        # n_glimpses x bs x C
        fts = torch.cat(fts)
        fts = fts.view(fts.shape[1], -1)
        Y_hat = self.final_head(fts)
        loss += self.loss(Y_hat, Y)
        return loss, Y_hat

    def training_step(self, batch, batch_nb):
        # REQUIRED
        X, Y = batch
        loss, _ = self.compute_loss(X, Y)
        lr = self.sched.optimizer.param_groups[-1]['lr']
        log = {'train_loss': loss, 'learning_rate': lr}
        return {'loss': loss, 'log': log}

    def validation_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        loss, y_hat = self.compute_loss(x, y)
        ret = {'val_loss': loss}
        n = y.shape[0]
        if self.hparams.loss == 'cross-entropy':
            y_hat = torch.softmax(y_hat, dim=1)
            y_hat = y_hat.argmax(dim=-1).view(n,-1)
        else:
            y_hat = torch.sigmoid(y_hat)
            y_hat = (y_hat > 0.5).view(n, -1)
        y = y.view(n,-1)
        ret['tp'] = ((y_hat)&(y==1)).float().sum()
        ret['tn'] = ((~y_hat)&(y==0)).float().sum()
        ret['fp'] = ((y_hat)&(y==0)).float().sum()
        ret['fn'] = ((~y_hat)&(y==1)).float().sum()
        return ret

    def validation_end(self, outputs):
        # OPTIONAL
        loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        log = {'val_loss': loss}
        tp = torch.stack([x['tp'] for x in outputs]).sum()
        fp = torch.stack([x['fp'] for x in outputs]).sum()
        tn = torch.stack([x['tn'] for x in outputs]).sum()
        fn = torch.stack([x['fn'] for x in outputs]).sum()
        for metric in self.metrics:
            try:
                name = metric.__name__
            except AttributeError:
                name = metric.func.__name__
                kws = metric.keywords
                for k in kws:
                    name += f'_{k}_{kws[k]}'
            log[name] = metric(tp, fp, tn, fn)
        return {'val_loss': loss, 'log': log}