---

title: Title
keywords: fastai
sidebar: home_sidebar

summary: "summary"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/25_models.losses.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="reduce" class="doc_header"><code>reduce</code><a href="https://github.com/schwobr/grade_classif/tree/master/grade_classif/models/losses.py#L9" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>reduce</code>(<strong><code>loss</code></strong>, <strong><code>reduction</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="focal_loss" class="doc_header"><code>focal_loss</code><a href="https://github.com/schwobr/grade_classif/tree/master/grade_classif/models/losses.py#L18" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>focal_loss</code>(<strong><code>input</code></strong>, <strong><code>target</code></strong>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong><code>beta</code></strong>=<em><code>0.5</code></em>, <strong><code>gamma</code></strong>=<em><code>2.0</code></em>, <strong><code>eps</code></strong>=<em><code>1e-07</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FocalLoss" class="doc_header"><code>class</code> <code>FocalLoss</code><a href="https://github.com/schwobr/grade_classif/tree/master/grade_classif/models/losses.py#L27" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FocalLoss</code>(<strong><code>beta</code></strong>=<em><code>0.5</code></em>, <strong><code>gamma</code></strong>=<em><code>2.0</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BCE" class="doc_header"><code>class</code> <code>BCE</code><a href="https://github.com/schwobr/grade_classif/tree/master/grade_classif/models/losses.py#L39" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BCE</code>(<strong><code>weight</code></strong>=<em><code>None</code></em>, <strong><code>size_average</code></strong>=<em><code>None</code></em>, <strong><code>reduce</code></strong>=<em><code>None</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong><code>pos_weight</code></strong>=<em><code>None</code></em>) :: <code>BCEWithLogitsLoss</code></p>
</blockquote>
<p>This loss combines a <code>Sigmoid</code> layer and the <code>BCELoss</code> in one single
class. This version is more numerically stable than using a plain <code>Sigmoid</code>
followed by a <code>BCELoss</code> as, by combining the operations into one layer,
we take advantage of the log-sum-exp trick for numerical stability.</p>
<p>The unreduced (i.e. with :attr:<code>reduction</code> set to <code>'none'</code>) loss can be described as:</p>
<p>.. math::
    \ell(x, y) = L = {l_1,\dots,l_N}^\top, \quad
    l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)</p>

<pre><code>+ (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],

</code></pre>
<p>where :math:<code>N</code> is the batch size. If :attr:<code>reduction</code> is not <code>'none'</code>
(default <code>'mean'</code>), then</p>
<p>.. math::
    \ell(x, y) = \begin{cases}
        \operatorname{mean}(L), &amp; \text{if reduction} = \text{'mean';}\\
        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{'sum'.}
    \end{cases}</p>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <code>t[i]</code> should be numbers
between 0 and 1.</p>
<p>It's possible to trade off recall and precision by adding weights to positive examples.
In the case of multi-label classification the loss can be described as:</p>
<p>.. math::
    \ell_c(x, y) = L<em>c = {l</em>{1,c},\dots,l<em>{N,c}}^\top, \quad
    l</em>{n,c} = - w_{n,c} \left[ p<em>c y</em>{n,c} \cdot \log \sigma(x_{n,c})</p>

<pre><code>+ (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],

</code></pre>
<p>where :math:<code>c</code> is the class number (:math:<code>c &gt; 1</code> for multi-label binary classification,
:math:<code>c = 1</code> for single-label binary classification),
:math:<code>n</code> is the number of the sample in the batch and
:math:<code>p_c</code> is the weight of the positive answer for the class :math:<code>c</code>.</p>
<p>:math:<code>p_c &gt; 1</code> increases the recall, :math:<code>p_c &lt; 1</code> increases the precision.</p>
<p>For example, if a dataset contains 100 positive and 300 negative examples of a single class,
then <code>pos_weight</code> for the class should be equal to :math:<code>\frac{300}{100}=3</code>.
The loss would act as if the dataset contains :math:<code>3\times 100=300</code> positive examples.</p>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10
&gt;&gt;&gt; output = torch.full([10, 64], 0.999)  # A prediction (logit)
&gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1
&gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
&gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(0.999))
tensor(0.3135)

</code></pre>
<p>Args:
    weight (Tensor, optional): a manual rescaling weight given to the loss
        of each batch element. If given, has to be a Tensor of size <code>nbatch</code>.
    size_average (bool, optional): Deprecated (see :attr:<code>reduction</code>). By default,
        the losses are averaged over each loss element in the batch. Note that for
        some losses, there are multiple elements per sample. If the field :attr:<code>size_average</code>
        is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored
        when reduce is <code>False</code>. Default: <code>True</code>
    reduce (bool, optional): Deprecated (see :attr:<code>reduction</code>). By default, the
        losses are averaged or summed over observations for each minibatch depending
        on :attr:<code>size_average</code>. When :attr:<a href="/models.losses.html#reduce"><code>reduce</code></a> is <code>False</code>, returns a loss per
        batch element instead and ignores :attr:<code>size_average</code>. Default: <code>True</code>
    reduction (string, optional): Specifies the reduction to apply to the output:
        <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
        <code>'mean'</code>: the sum of the output will be divided by the number of
        elements in the output, <code>'sum'</code>: the output will be summed. Note: :attr:<code>size_average</code>
        and :attr:<a href="/models.losses.html#reduce"><code>reduce</code></a> are in the process of being deprecated, and in the meantime,
        specifying either of those two args will override :attr:<code>reduction</code>. Default: <code>'mean'</code>
    pos_weight (Tensor, optional): a weight of positive examples.
            Must be a vector with length equal to the number of classes.</p>
<p>Shape:</p>

<pre><code>- Input: :math:`(N, *)` where :math:`*` means, any number of additional dimensions
- Target: :math:`(N, *)`, same shape as the input
- Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same
  shape as input.

</code></pre>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()
&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)
&gt;&gt;&gt; target = torch.empty(3).random_(2)
&gt;&gt;&gt; output = loss(input, target)
&gt;&gt;&gt; output.backward()</code></pre>

</div>

</div>

</div>
</div>

</div>
</div>
 

